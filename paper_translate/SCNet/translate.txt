SCNet: Learning Semantic Correspondence 

SCNet：学习语义对应



Kai Han, Rafael Rezende, Bumsub Ham, Kwan-Yee Wong, Minsu Cho, Cordelia Schmid, Jean Ponce 

Kai Han，Rafael Rezende，Bumsub Ham，Kwan-Yee Wong，Choi Minsu，Cordelia Schmid，Jean Ponce



To cite this version: 

引用这个版本：



Kai Han, Rafael Rezende, Bumsub Ham, Kwan-Yee Wong, Minsu Cho, et al.. SCNet: Learning Semantic Correspondence. International Conference on Computer Vision, Oct 2017, Venise, Italy. 2017, International conference on computer vision. <hal-01576117> 

Kai Han，Rafael Rezende，Bumsub Ham，Kwan-Yee Wong，Minsu Cho等人。SCNet：学习语义对应。计算机视觉国际会议，2017年10月，威尼斯，意大利。 2017，计算机视觉国际会议。 \x26lt



HAL Id: hal-01576117 https://hal.archives-ouvertes.fr/hal-01576117 

HAL Id：hal-01576117 https://hal.archives-ouvertes.fr/hal-01576117



Submitted on 22 Aug 2017 

2017年8月22日提交



HAL is a multi-disciplinary open access archive for the deposit and dissemination of scientific research documents, whether they are published or not. The documents may come from teaching and research institutions in France or abroad, or from public or private research centers. 

HAL是一个多学科的开放式访问档案，用于存放和传播科学研究文档，不管它们是否已发布。这些文件可能来自法国或国外的教学和研究机构，或来自公立或私立研究中心。



L’archive ouverte pluridisciplinaire HAL, est destinée au dépôt et à la diffusion de documents scientifiques de niveau recherche, publiés ou non, émanant des établissements d’enseignement et de recherche français ou étrangers, des laboratoires publics ou privés. 

L\x26#39



SCNet: Learning Semantic Correspondence 

SCNet：学习语义对应



Kai Han1 

凯涵1



Rafael S. Rezende4,5 Bumsub Ham2 Kwan-Yee K. Wong1 Minsu Cho3 Cordelia Schmid4, ∗ Jean Ponce5,4 1The University of Hong Kong 2Yonsei Univ. 3POSTECH 4 Inria 5Department of Computer Science, ENS / CNRS / PSL Research University 

Rafael S. Rezende4,5 Bumsub Ham2 Kwan-Yee K. Wong1 Minsu Cho3 Cordelia Schmid4，* Jean Ponce5,4 1香港大学2Yonsei Univ。 3POSTECH 4 Inria 5计算机科学系，ENS / CNRS / PSL研究大学



Abstract 

抽象



This paper addresses the problem of establishing semantic correspondences between images depicting different instances of the same object or scene category. Previous approaches focus on either combining a spatial regularizer with hand-crafted features, or learning a correspondence model for appearance only. We propose instead a convolutional neural network architecture, called SCNet, for learning a geometrically plausible model for semantic correspondence. SCNet uses region proposals as matching primitives, and explicitly incorporates geometric consistency in its loss function. It is trained on image pairs obtained from the PASCAL VOC 2007 keypoint dataset, and a comparative evaluation on several standard benchmarks demonstrates that the proposed approach substantially outperforms both recent deep learning architectures and previous methods based on hand-crafted features. 

本文讨论了在描述同一对象或场景类别的不同实例的图像之间建立语义对应关系的问题。先前的方法着重于将空间正规化器与手工制作的特征相结合，或者仅为外观学习对应模型。我们提出了一种叫做SCNet的卷积神经网络体系结构，用于学习语义对应的几何可信模型。 SCNet使用区域提议作为匹配基元，并明确地将几何一致性纳入其损失函数中。它是从PASCAL VOC 2007关键点数据集中获得的图像对进行培训的，对几个标准基准的比较评估表明，所提出的方法远远优于近期深度学习架构和基于手工特征的以前方法。



1. Introduction 

1.介绍



Our goal in this paper is to establish semantic correspondences across images that contain different instances of the same object or scene category, and thus feature much larger changes in appearance and spatial layout than the pictures of the same scene used in stereo vision, which we take here to include broadly not only classical (narrow-baseline) stereo fusion (e.g., [31, 34]), but also optical ﬂow computation (e.g., [15, 33, 42]) and wide-baseline matching (e.g., [30, 43]). Due to such a large degree of variations, the problem of semantic correspondence remains very challenging. Most previous approaches to semantic correspondence [2, 17, 20, 26, 37, 43] focus on combining an effective spatial regularizer with hand-crafted features such as SIFT [28], DAISY [39] or HOG [6]. With the remarkable success of deep learning approaches in visual recognition, several learning-based methods have also been proposed for 

我们在本文中的目标是在包含同一对象或场景类别的不同实例的图像之间建立语义对应关系，并因此在外观和空间布局方面比在立体视觉中使用的相同场景的图像具有更大的变化，这里广泛地不仅包括经典（窄基线）立体融合（例如[31,34]），还包括光流计算（例如[15,33,42]）和宽基线匹配（例如[30 ，43]）。由于如此大的变化，语义对应的问题仍然非常具有挑战性。大多数先前的语义对应方法[2,17,20,26,37,43]集中于将有效的空间正则化器与手工特征如SIFT [28]，DAISY [39]或HOG [6]相结合。随着视觉识别中的深度学习方法的显着成功，已经提出了几种基于学习的方法



∗Univ. Grenoble Alpes, Inria, CNRS, Grenoble INP, LJK, 38000 Grenoble, France. 

*大学。格勒诺布尔阿尔卑斯，Inria，CNRS，格勒诺布尔INP，LJK，法国格勒诺布尔38000。



Figure 1: Learning semantic correspondence. We propose a convolutional neural network, SCNet, to learn semantic correspondence using both appearance and geometry. This allows us to handle a large degree of intra-class and scene variations. This ﬁgure shows a pair of input images (top) and a warped image (bottom) using its semantic correspondence by our method. (Best viewed in color.) 

图1：学习语义对应。我们提出了一个卷积神经网络SCNet，使用外观和几何学来学习语义对应。这使我们能够处理很大程度的课堂和场景变化。该图使用我们的方法使用其语义对应来显示一对输入图像（顶部）和变形图像（底部）。 （最好用彩色。）



both stereo vision [9, 12, 47, 48] and semantic correspondence [5, 21, 50]. Yet, none of these methods exploits the geometric consistency constraints that have proven to be a key factor to the success of their hand-crafted counterparts. Geometric regularization, if any, occurs during postprocessing but not during learning (e.g., [47, 48]). In this paper we propose a convolutional neural network (CNN) architecture, called SCNet, for learning geometrically plausible semantic correspondence (Figure 1). Following the proposal ﬂow approach to semantic correspondence of Ham et al. [11], we use object proposals [29, 40, 51] as matching primitives, and explicitly incorporate the geometric consistency of these proposals in our loss function. Unlike [11] with its hand-crafted features, however, we train our system in an end-to-end manner using image pairs extracted from the PASCAL VOC 2007 keypoint dataset [7]. A comparative evaluation on several 

包括立体视觉[9,12,47,48]和语义对应[5,21,50]。然而，这些方法都没有利用几何一致性约束，这些约束已被证明是手工制作对手成功的关键因素。几何正则化（如果有的话）在后处理期间发生，但在学习期间不发生（例如，[47,48]）。在本文中，我们提出了一种卷积神经网络（CNN）架构，称为SCNet，用于学习几何上合理的语义对应（图1）。遵循Ham等人提出的语义对应的流程方法。 [11]，我们使用对象提议[29,40,51]作为匹配原语，并明确地将这些提议的几何一致性纳入我们的损失函数中。然而，与其具有手工特征的[11]不同，我们使用从PASCAL VOC 2007关键点数据集中提取的图像对以端对端的方式训练我们的系统[7]。对几个问题的比较评估



1 

1



standard benchmarks demonstrates that the proposed approach substantially outperforms both recent deep architectures and previous methods based on hand-crafted features. Our main contributions can be summarized as follows: • We introduce a simple and efﬁcient model for learning to match regions using both appearance and geometry. • We propose a convolutional neural network, SCNet, to learn semantic correspondence with region proposals. • We achieve state-of-the-art results on several benchmarks, clearly demonstrating the advantage of learning both appearance and geometric terms. 

标准基准测试表明，所提出的方法远远优于近期的深度架构和基于手工特性的以前的方法。我们的主要贡献可概括如下：•我们引入了一个简单而有效的学习模型，用于匹配使用外观和几何的区域。 •我们提出一个卷积神经网络SCNet，以学习与区域提案的语义对应关系。 •我们在几项基准测试中取得了最先进的成果，清楚地表明了学习外观和几何术语的优势。



2. Related work 

2.相关工作



Here we brieﬂy describe representative approaches related to semantic correspondence. Semantic correspondence. SIFT Flow [26] extends classical optical ﬂow to establish correspondences across similar but different scenes. It uses dense SIFT descriptors to capture semantic information beyond naive color values, and leverages a hierarchical optimization technique in a coarse-to-ﬁne pipeline for efﬁciency. Kim et al. [20] and Hur et al. [17] propose more efﬁcient generalizations of SIFT Flow. Instead of using SIFT features, Yang et al. [43] use DAISY [39] for an efﬁcient descriptor extraction. Inspired by an exemplar-LDA approach [13], Bristow et al. [2] use whitened SIFT descriptors, making semantic correspondence robust to background clutter. Recently, Ham et al. [11] introduces proposal ﬂow that uses object proposals as matching elements for semantic correspondence robust to scale and clutter. This work shows that the HOG descriptor gives better matching performance than deep learning features [23, 35]. Taniai et al. [37] also use HOG descriptors, and show that jointly performing cosegmentation and establishing dense correspondence are helpful in both tasks. Despite differences in feature descriptors and optimization schemes, these semantic correspondence approaches use a spatial regularizer to ensure ﬂow smoothness on top of hand-crafted or pre-trained features. 

在这里，我们简要描述了与语义对应相关的代表性方法。语义对应。 SIFT Flow [26]扩展了经典的光学PャPw以建立类似但不同场景的对应关系。它使用密集的SIFT描述符来捕捉超出朴素颜色值的语义信息，并利用粗糙到 流水线中的分层优化技术来提高效率。 Kim等人[20]和Hur等人。 [17]提出了更多有效的SIFT流的概括。杨等人不使用SIFT特征， [43]使用DAISY [39]进行有效的描述符提取。受到典型-LDA方法的启发[13]，Bristow等人[2]使用白化的SIFT描述符，使语义对应对背景混乱具有鲁棒性。最近，Ham等人[11]介绍了使用对象提议作为语义对应的匹配元素的建议，该语义对应对于规模和杂乱是鲁棒的。这项工作表明，HOG描述符比深度学习功能提供更好的匹配性能[23,35]。 Taniai等人[37]也使用HOG描述符，并表明联合执行cosegmentation和建立密集的对应关系在两个任务中都有帮助。尽管特征描述符和优化方案存在差异，但这些语义对应方法使用空间正则化器来确保手工制作或预先训练的特征上的平滑性。



Deep learning for correspondence. Recently, CNNs 

深入学习通信。最近，CNN



have been applied to classical dense correspondence problems such as optical ﬂow and stereo matching to learn feature descriptors [46, 47, 48] or similarity functions [12, 46, 47]. FlowNet [9] uses an end-to-end scheme to learn optical ﬂow with a synthetic dataset, and several recent approaches also use supervision from reconstructed 3D scenes and stereo pairs [12, 46, 47, 48]. MC-CNN [47] and its efﬁcient extension [48] train CNN models to predict how well two image patches match and use this information to compute the stereo matching cost. DeepCompare [46] learns a similarity function for patches directly from images of a 3D scene, which allows for various types of geometric 

已被应用于经典密集对应问题，如光流和立体匹配以学习特征描述符[46,47,48]或相似函数[12,46,47]。 FlowNet [9]采用端到端的方案来学习合成数据集的光流，最近的几种方法也使用重建的3D场景和立体对的监督[12,46,47,48]。 MC-CNN [47]及其有效的扩展[48]训练CNN模型以预测两个图像块的匹配程度，并使用这些信息来计算立体匹配成本。 DeepCompare [46]直接从3D场景的图像中学习贴片的相似性函数，这允许各种类型的几何



and photometric transformations (e.g., rotation and illumination changes). These approaches are inherently limited to matching images of the same physical object/scene. In contrast, Long et al. [27] use CNN features pre-trained for ImageNet classiﬁcation tasks (due to a lack of available datasets for learning semantic correspondence) with performance comparable to SIFT ﬂow. To overcome the difﬁculty in obtaining ground truth for semantic correspondence, Zhou et al. [50] leverage 3D models, and uses ﬂow consistency between 3D models and 2D images as a supervisory signal to train a CNN. Another approach to generating ground truth is to directly augment the data by densifying sparse keypoint annotations using warping [11, 18]. The universal correspondence network (UCN) of Choy et al. [5] learns semantic correspondence using an architecture similar to [48], but adds a convolutional spatial transformer networks for improved robustness to rotation and scale changes. Kim et al. [21] introduce a convolutional descriptor using self-similarity, called fully convolutional self-similarity (FCSS), and combine the learned semantic descriptors with the proposal ﬂow [11] framework. These approaches to learning semantic correspondence [5, 50] or semantic descriptors [21] typically perform better than traditional hand-crafted ones. Unlike our method, however, they do not incorporate geometric consistency between regions or object parts in the learning process. 

和光度变换（例如，旋转和照明变化）。这些方法固有地局限于匹配相同物理对象/场景的图像。相反，Long等人[27]使用预先训练过的用于ImageNet分类任务的CNN特征（由于缺乏用于学习语义对应的可用数据集），其性能可与SIFT ャPw相媲美。为了克服获得语义对应的基本事实的不同难点，Zhou等人[50]利用3D模型，并使用3D模型和2D图像之间的Pw一致性作为监督信号来训练CNN。另一种生成基本事实的方法是通过使用翘曲来稀疏化关键点注释来直接增加数据[11,18]。 Choy等人的通用通信网络（UCN） [5]使用类似于[48]的体系结构学习语义对应关系，但增加了卷积空间变换网络以提高旋转和缩放变化的鲁棒性。 Kim等人[21]引入使用自相似性的卷积描述符，称为完全卷积自相似（FCSS），并将学习过的语义描述符与提议框架[11]结合起来。这些学习语义对应[5,50]或语义描述[21]的方法通常比传统的手工方法表现得更好。然而，与我们的方法不同的是，它们没有在学习过程中将区域或对象部分之间的几何一致性结合起来



3. Our approach 

3.我们的方法



We consider the problem of learning to match regions with arbitrary positions and sizes in pairs of images. This setting is general enough to cover all cases of region sampling used in semantic correspondence: sampling a dense set of regular local regions as in typical dense correspondence [2, 20, 26, 38] as well as employing multi-scale object proposals [1, 16, 29, 40, 51]. In this work, following proposal ﬂow [11], we focus on establishing correspondences between object proposal boxes. 

我们认为学习的问题是在成对的图像中匹配具有任意位置和大小的区域。这种设置通常足以涵盖在语义对应中使用的区域抽样的所有情况：如在典型的密集对应中采样密集的规则局部区域[2,20,26,38]以及采用多尺度对象提议[1 ，16,29,40,51]。在这项工作中，根据提案流程[11]，我们着重于建立对象提案框之间的对应关系。



3.1. Model 

3.1。模型



Our basic model for matching starts from the probabilistic Hough matching (PHM) approach of [4, 11]. In a nutshell, given some potential match m between two regions, and the supporting data D (a set of potential matches), the PHM model can be written as 

我们的匹配基本模型从[4,11]的概率霍夫匹配（PHM）方法开始。简而言之，考虑到两个地区之间潜在的匹配度m和支持数据D（一组潜在的匹配），PHM模型可写成



P (m|D) = 

P（m | D）\x3d



P (m|x, D)P (x|D) 

P（m | x，D）P（x | D）



(cid:88) 

（CID：88）



(cid:88) 

（CID：88）



x 

X



= Pa (m) 

\x3d Pa（m）



Pg (m|x)P (x|D), 

Pg（m | x）P（x | D），



(1) 

（1）



x 

X



where x is the offset (e.g., position and scale change) between all potential matches m = [r, s] of two regions r and s. Pa (m) is the probability that the match between two 

其中x是两个区域r和s的所有潜在匹配m \x3d [r，s]之间的偏移（例如，位置和比例变化）。 Pa（m）是两个之间匹配的概率



regions is correct based on appearance only, and Pg (m|x) is the probability based on geometry only, computed using the offset x1 . PHM computes a matching score by replacing geometry prior P (x|D) with the Hough voting h(x|D) [4]: (2) 

基于外观的区域是正确的，并且Pg（m | x）是仅基于几何体的概率，使用偏移量x1计算。 PHM通过用Hough投票h（x | D）[4]代替几何先验P（x | D）来计算匹配分数：（2）



Pa (m(cid:48) )Pg (m(cid:48) |x). 

Pa（m（cid：48））Pg（m（cid：48）| x）。



h(x|D) = 

h（x | D）\x3d



(cid:88) 

（CID：88）



m(cid:48)∈D 

米（CID：48）∈D



This turns out to be an effective spatial matching model that combines appearance similarity with global geometric consistency measured by letting all matches vote on the potential offset x [4, 11]. In our learning framework, we consider similarities rather than probabilities, and rewrite the PHM score for the match m as 

事实证明，这是一种有效的空间匹配模型，它将外观相似性与全局几何一致性相结合，通过让所有匹配对潜在偏移量进行投票来测量[4,11]。在我们的学习框架中，我们考虑相似性而不是概率，并重写匹配m的PHM分数



(cid:88) 

（CID：88）



z (m, w) = f (m, w) 

z（m，w）\x3d f（m，w）



(cid:88) 

（CID：88）



f (m(cid:48) , w)g(m(cid:48) , x) g(m, x) g(m, x)g(m(cid:48) , x)]f (m(cid:48) , w), 

f（m（cid：48），w）g（m（cid：48），x）g（m，x）g（m，x）g（m（cid：48），x）] cid：48），w），



m(cid:48)∈D 

米（CID：48）∈D



[ 

[



(cid:88) (cid:88) 

（cid：88）（cid：88）



x 

X



= f (m, w) 

\x3d f（m，w）



m(cid:48)∈D 

米（CID：48）∈D



x 

X



(3) where f (m, w) is a parameterized appearance similarity function between the two regions in the potential match m, x is as before an offset variable (position plus scale), and g(m, x) measures the geometric compatibility between the match m and the offset x. Now assuming that we have a total number of n potential matches, and identifying matches with their indices, we can rewrite this score as 

（3）其中f（m，w）是潜在匹配m中两个区域之间的参数化外观相似度函数，x与偏移量变量（位置加标度）相同，g（m，x）测量几何兼容性匹配m和偏移x之间。现在假设我们有n个潜在匹配的总数，并且标识与它们的索引匹配，我们可以将该分数重写为



z (m, w) = f (m, w) 

z（m，w）\x3d f（m，w）



Kmm(cid:48) f (m(cid:48) , w), 

Kmm（cid：48）f（m（cid：48），w），



g(m, x)g(m(cid:48) , x), 

g（m，x）g（m（cid：48），x），



where Kmm(cid:48) = (4) and the n × n matrix K is the kernel matrix associated 

其中Kmm（cid：48）\x3d（4）并且n×n矩阵K是关联的核矩阵



x 

X



with the feature vector ϕ(m) = [g(m, x1 ), . . . , g(m, xs )]T , 

与特征向量φ（m）\x3d [g（m，x1），...， 。 。 ，g（m，xs）] T，



where x1 to xs form the ﬁnite set of values that the offset variable x runs over: indeed Kmm(cid:48) = ϕ(m) · ϕ(m(cid:48) ).2 Given training pairs of images with associated true and false matches, we can learn our similarity function by minimizing with respect to w 

其中x1到xs形成偏移变量x遍历的有限值集合：事实上Kmm（cid：48）\x3dφ（m）·φ（m（cid：48））。和错误匹配，我们可以通过关于w的最小化来学习我们的相似性函数



(cid:88) (cid:88) 

（cid：88）（cid：88）



m(cid:48) 

米（CID：48）



n(cid:88) 

N（CID：88）



E (w) = 

E（w）\x3d



l[ym , z (m, w)] + λΩ(w), 

1 [ym，z（m，w）] +λΩ（w），



(5) 

（5）



m=1 

m \x3d 1的



where l is a loss function, ym is the the ground-truth label (either 1 [true] or 0 [false]) for the match m, and Ω is a regularizer (e.g., Ω(w) = ||w||2 ). We use the hinge loss and L2 regularizer in this work. Finally, at test time, we associate any region r with the region s maximizing z ([r, s], w∗ ), where w∗ is the set of learned parameters. 

其中l是损失函数，ym是匹配m的地面实况标签（1 [true]或0 [false]），Ω是正则化函数（例如Ω（w）\x3d || w ||）。 2）。我们在这项工作中使用铰链损失和L2调整器。最后，在测试时，我们将任意区域r与区域s（[r，s]，w *）最大化相关联，其中w *是学习参数的集合。



1We suppose that appearance matching is independent of geometry matching and the offset. 2 Putting it all together in an n-vector of scores, this can also be rewrit“(cid:12)” stands for the elementwise product between vectors, and f (w) = 

1我们假设外观匹配与几何匹配和偏移无关。 2将它们放在一个n向量中，也可以重写为“（cid：12）”代表向量之间的元素乘积，f（w）\x3d



ten as z (w) = f (w)(cid:12)K f (w), where z (w) = (z (1, w), . . . , z (n, w))T , 

作为z（w）\x3d f（w）（cid：12）K f（w），其中z（w）\x3d（z（1，w），...，z（n，w）



(f (1, w), . . . , f (n, w))T . 

（f（1，w），...，f（n，w））T。



(7) 

（7）



(8) 

（8）



3.2. Similarity function and geometry kernel 

3.2。相似函数和几何内核



f (m, w) = max(0, c(r, w) · c(s, w)), 

f（m，w）\x3d max（0，c（r，w）·c（s，w）），



There are many possible choices for the function f that computes the appearance similarity of the two regions r and s making up match number m. Here we assume a trainable embedding function c (as will be shown later, c will be the output of a CNN in our case) that outputs a L2 normalized feature vector. For the appearance similarity between two regions r and s, we then use a rectiﬁed cosine similarity: (6) that sets all negative similarity values to zero, thus making the similarity function sparser as well as insensitive to negative matches during training, with the additional beneﬁt of giving nonnegative weights in Eq. (3). Our geometry kernel Kmm(cid:48) records the fact that two matches (roughly) correspond to the same offset: Concretely, we discretize the set of all possible offsets into bins. Let us denote by h the function mapping a match m onto the corresponding bin x, we now deﬁne g by 

计算两个区域r和s的外观相似度的函数f有很多可能的选择，组成匹配数m。这里我们假设一个可训练的嵌入函数c（如后面所示，c将是我们的情况下CNN的输出），它输出一个L2归一化特征向量。对于两个区域r和s之间的外观相似性，我们使用整型余弦相似度：（6）将所有负相似度值设为零，从而使得相似度函数更稀疏，并且对训练期间的负匹配不敏感，在方程式中给出非负权重的好处。 （3）。我们的几何核心Kmm（cid：48）记录了两个匹配（大致）对应于相同偏移量的事实：具体地说，我们将所有可能的偏移集合离散化为分组。让我们用h来表示将匹配m映射到相应的bin x上的函数，现在我们用g来定义



1, 

1，



if h(m) = x 

如果h（m）\x3d x



g(m, x) = 

g（m，x）\x3d



0, otherwise. Thus, the kernel Kmm(cid:48) simply measures whether two matches share the same offset bin or not: 

0，否则。因此，内核Kmm（cid：48）简单地测量两个匹配是否共享相同的偏移量bin：



(cid:40) 

（CID：40）



(cid:40) 

（CID：40）



if h(m) = h(m(cid:48) ) 

如果h（m）\x3d h（m（cid：48））



1, 

1，



Kmm(cid:48) = 

Kmm（cid：48）\x3d



0, otherwise. In practice, x runs over a grid of predeﬁned offset values, and h(m) assigns match m to the nearest offset point. Our kernel is sparse, which greatly simpliﬁes the computation of the score function in Eq. (4): Indeed, let Bx denote the set of matches associated with the bin x, the score function z reduces to 

0，否则。实际上，x运行在预定义的偏移值的网格上，h（m）将匹配m分配给最近的偏移点。我们的核心是稀疏的，这极大地简化了等式（2）中得分函数的计算。 （4）：的确，让Bx表示与x相关联的匹配集合，分数函数z简化为



z (m, w) = f (m, w) 

z（m，w）\x3d f（m，w）



f (m(cid:48) , w). 

f（m（cid：48），w））。



(9) 

（9）



(cid:88) 

（CID：88）



m(cid:48)∈Bh(m) 

米（CID：48）∈Bh（M）



This trainable form of the PHM model from [4, 11] can be term value of (cid:80) used within Eq. (5). Note that since our simple geometry kernel is only dependent on matches’ offsets, we obtain the same geometry f (m(cid:48) , w) for any match m that falls into the same bin h(m). This allows us to compute this geometry term value only once for each non-empty bin x and then share it for multiple matches in the same bin. This sharing makes computing z several times faster in practice.3 

这种来自[4,11]的PHM模型的可训练形式可以是公式（5）中使用的（cid：80）的term值。 （5）。请注意，由于我们的简单几何内核仅取决于匹配的偏移量，因此对于落入同一个bin h（m）的任何匹配m，我们获得相同的几何结构f（m（cid：48），w）。这允许我们为每个非空的箱x计算这个几何项值只有一次，然后在同一个箱中共享多个匹配。这种共享使得计算在实践中快几倍



m(cid:48)∈Bh(m) 

米（CID：48）∈Bh（M）



3.3. Gradient-based learning 

3.3。基于渐变的学习



The feature embedding function c(m, w) in the model above can be implemented by any differentiable architecture, for example a CNN-based one, and the score function 

上述模型中的特征嵌入函数c（m，w）可以通过任何可微分体系结构来实现，例如基于CNN的体系结构，并且分数函数



3 If the geometry kernel is dependent on something other than offsets, e.g., matches’ absolute position or their neighborhood structure, this sharing is not possible. 

3如果几何内核依赖于偏移以外的其他内容，例如匹配绝对位置或其邻域结构，则这种共享是不可能的。



Figure 2: The SCNet architectures. Three variants are proposed: SCNet-AG, SCNet-A, and SCNet-AG+. The basic architecture, SCNet-AG, is drawn in solid lines. Colored boxes represent layers with learning parameters and the boxes with the same color share the same parameters. “×K ” denotes the voting layer for geometric scoring. A simpliﬁed variant, SCNet-A, learns appearance information only by making the voting layer an identity function. An extended variant, SCNet-AG+, contains an additional stream drawn in dashed lines. SCNet-AG learns a single embedding c for both appearance and geometry, whereas SCNet-AG+ learns an additional and separate embedding cg for geometry. See text for details. (Best viewed in color.) 

图2：SCNet架构。提出了三种变体：SCNet-AG，SCNet-A和SCNet-AG +。基本架构SCNet-AG采用实线绘制。彩色框表示具有学习参数的图层，并且具有相同颜色的框共享相同的参数。 “×K”表示几何得分的投票层。简化的变体SCNet-A仅通过使投票层成为身份函数来学习外观信息。一个扩展的变体SCNet-AG +包含一个以虚线绘制的附加流。 SCNet-AG为外观和几何学习单个嵌入c，而SCNet-AG +为几何学习另外并分离嵌入cg。详情请参阅文字。 （最好用彩色。）



z can be learned using stochastic gradient descent. Let us now consider the problem of minimizing the objective function E (w) deﬁned by Eq. (5).4 This requires computing the gradient with respect to w of the function z : 

可以使用随机梯度下降来学习z。现在让我们考虑最小化由方程式定义的目标函数E（w）的问题。 （5）.4这需要计算关于函数z的w的梯度：



∇z (m, w) = [ 

∇z（m，w）\x3d [



Kmm(cid:48) f (m(cid:48) , w)]∇f (m, w) 

Kmm（cid：48）f（m（cid：48），w）]∇f（m，w）



+f (m, w) 

+ f（m，w）



Kmm(cid:48) ∇f (m(cid:48) , w). 

Kmm（cid：48）∇f（m（cid：48），w）。



(10) 

（10）



(cid:88) 

（CID：88）



m(cid:48)∈D 

米（CID：48）∈D



(cid:88) 

（CID：88）



m(cid:48)∈D 

米（CID：48）∈D



Denoting by n the size of D , this involves n evaluations of both f and ∇f . Computing the full gradient of E thus requires at most n2 evaluations of both f and ∇f , which becomes computationally intractable when n is large enough. The score function of Eq. (9) with the sparse kernel of Eq. (8), however, greatly reduces the gradient computation: 

用n表示D的大小，这涉及对f和∇f的n个评估。因此计算E的完整梯度因此最多需要n 2个f和∇f的评估，当n足够大时，这在计算上变得难以处理。方程的得分函数（9）式的稀疏内核。 （8）然而，大大减少了梯度计算：



∇z (m, w) = [ 

∇z（m，w）\x3d [



f (m(cid:48) , w)]∇f (m, w) 

f（m（cid：48），w）]∇f（m，w）



(cid:88) 

（CID：88）



m(cid:48)∈Bh(m) 

米（CID：48）∈Bh（M）



+f (m, w) 

+ f（m，w）



(cid:88) 

（CID：88）



m(cid:48)∈Bh(m) 

米（CID：48）∈Bh（M）



∇f (m(cid:48) , w). 

∇f（m（cid：48），w））。



(11) 

（11）



Note that computing the gradient for match m involves only a small set of matches falling into the same offset bin h(m). 

请注意，计算匹配m的梯度只涉及一小组匹配落入相同的偏移bin h（m）。



4. SCNet architecture 

4. SCNet架构



Among many possible architectures implementing the proposed model, we propose using a convolutional neural network (CNN), dubbed SCNet, that efﬁciently processes 

在实现所提出的模型的许多可能的体系结构中，我们建议使用称为SCNet的卷积神经网络（CNN），其有效地处理



4We take Ω(w) = 0 for simplicity in this section, but tackling a nonzero regularizer is easy. 

4本节中为了简化，我们取Ω（w）\x3d 0，但是处理非零正则化变得容易。



regions and learns our matching model. Three variants, SCNet-AG, SCNet-A, SCNet-AG+, are illustrated in Fig. 2. In each case, SCNet takes as input two images IA and IB , and maps them onto feature maps FA and FB by CNN layers. Given region proposals (r1 , . . . , rp ) and (s1 , . . . , sp ) for the two images, parallel ROI pooling layers [10, 14] extract feature maps of the same size for each proposal. This is an efﬁcient architecture that shares convolutional layers over all region proposals. 

并且学习我们的匹配模型。 SCNet-AG，SCNet-A，SCNet-AG +三种变型如图2所示。在每种情况下，SCNet将输入两个图像IA和IB作为输入，并通过CNN层将它们映射到特征映射FA和FB。给定两个图像的区域提议（r1，...，rp）和（s1，...，sp），并行ROI池层[10,14]为每个提议提取相同大小的特征映射。这是一个有效的架构，它共享所有区域提案的卷积层。



SCNet-AG. The proposal features are fed into a fullyconnected layer, mapped onto feature embedding vectors, and normalized into unit feature vectors c(ri , w) and c(sj , w), associated with the regions ri and sj of IA and IB , respectively. The value of f (m, w) for the match m associated with regions ri and sj is computed as the rectiﬁed dot product of c(ri ) and c(sj ) (Eq. (6)), which deﬁnes the appearance similarity f (m, w) for match m. Geometric consistency is enforced with the kernel described in Sec. 3.2, using a voting layer, denoted as “×K ”, that computes score z (m, w) from the appearance similarity and geometric consensus of proposals. Finally, matching is performed by identifying the maximal z (m, w) scores, using both appearance and geometric similarities. 

SCNet-AG。提议特征被馈送到完全连接层中，映射到特征嵌入向量，并且被归一化为分别与IA和IB的区域ri和sj相关联的单位特征向量c（ri，w）和c（sj，w）。与区域ri和sj相关联的匹配m的f（m，w）的值被计算为c（ri）和c（sj）（等式（6））的整数点积，其定义了外观相似度f （m，w）表示匹配m。几何一致性与第二部分描述的内核一起实施。 3.2，使用投票层，表示为“×K”，根据提案的外观相似度和几何一致性计算得分z（m，w）。最后，通过使用外观和几何相似性来识别最大z（m，w）分数来执行匹配。



SCNet-A. We also evaluate a similar architecture without the geometry term. This architecture drops the voting layer (denoted by ×K in Fig. 2) from SCNet-AG, directly using f (m, w) as a score function. This is similar to the universal correspondence network (UCN) [5]. The main differences are the use of object proposals and the use of a different loss function. 

SCNet-A。我们还评估一个没有几何术语的类似架构。该架构直接使用f（m，w）作为分数函数，从SCNet-AG中删除投票层（图2中用×K表示）。这与通用通信网络（UCN）类似[5]。主要区别是使用对象提议和使用不同的损失函数。



SCNet-AG+. Unlike SCNet-AG, which learns a single embedding c for both appearance and geometry, SCNetAG+ learns an additional and separate embedding cg for geometry that is implemented by an additional stream in the SCNet architecture (dashed lines in Fig. 2). This corresponds to a variant of Eq. (9), as follows: 

SCNet-AG +。不同于SCNet-AG，它学习了外观和几何的单个嵌入c，SCNetAG +学习了一个额外的，独立的嵌入cg，用于由SCNet架构中的附加流实现的几何图形（图2中的虚线）。这对应于方程式的变体。 （9）如下：



(cid:88) 

（CID：88）



m(cid:48)∈Bh(m) 

米（CID：48）∈Bh（M）



z+ (m, w) = f (m, w) 

z +（m，w）\x3d f（m，w）



fg (m(cid:48) , w), 

fg（m（cid：48），w），



(12) 

（12）



where fg is the rectiﬁed cosine similarity computed by cg . Compared to the original score function, this variant allows the geometry term to learn a separate embedding function for geometric scoring. This may be beneﬁcial particularly when a match’s contribution to geometric scoring needs to be different from the appearance score. For example, a match of rigid object parts (wheel of cars) may contribute more to geometric scoring than that of deformable object parts (leg of horses). The separate similarity function fg allows more ﬂexibility in learning the geometric term. 

fg是由cg计算的整除余弦相似度。与原始分数函数相比，此变体允许几何术语学习几何计分的单独嵌入函数。这可能是有益的，特别是当比赛对几何得分的贡献需要与出场得分不同时。例如，刚性物体部分（汽车轮）的匹配可能比可变形物体部分（马腿）更有助于几何得分。单独的相似度函数fg允许学习几何项的灵活性更高。



Implementation details. We use the VGG16 [36] model 

实施细节。我们使用VGG16 [36]模型



that consists of a set of convolutional layers with 3 × 3 ﬁlters, a ReLU layer and a pooling layer. We ﬁnd that taking the ﬁrst 4 convolutional layers is a good trade-off for our semantic feature extraction purpose without loosing localization accuracy. These layers output features with 512 channels. For example, if the net takes input of 224×224×3 images, the convolutional layers produce features with the size of 14 × 14 × 512. For the ROI pooling layer, we choose a 7 × 7 ﬁlter following the fast R-CNN architecture [10], which produces a feature map with size of 7 × 7 × 512 for each proposal. To transform the feature map for each proposal into a feature vector, we use the F C layer with a size of 7 × 7 × 512 × 2048. The 2048 dimensional feature vector associated with each proposal are then fed into the L2 normalization layer, followed by the dot product layer, ReLU, our geometric voting layer, and loss layer. The convolutional layers are initialized by the pretrained weights of VGG16 and the fully connected layers have random initialization. We train our SCNet by mini-batch SGD, with learning rate 0.001, and weight decay 0.0005. During training, each mini-batch arises from a pair of images associated with a number of proposals. In our implementation, we generated 500 proposals for each image, which leads to 500 × 500 potential matches. For each mini-batch, we sample matches for training as follows. (1) Positive sampling: For a proposal ri in IA , we are given its ground truth match r (cid:48) i in IB . We pick all the proposals sj in IB with IoU(sj , r (cid:48) i ) > Tpos to be positive matches for ri . (2) Negative sampling: Assume we obtain k positive pairs w.r.t ri . We also need to have k negative pairs w.r.t ri . To achieve this, we ﬁrst ﬁnd the proposals i ) < Tneg . Assuming p proposals 

它由一组具有3×3滤波器的卷积层，一个ReLU层和一个汇聚层组成。我们发现，对于我们的语义特征提取目的而言，采用前4个卷积层是一个很好的折衷，而不会失去定位精度。这些图层输出具有512个通道的功能。例如，如果网络输入224×224×3的图像，卷积层产生大小为14×14×512的特征。对于ROI池层，我们选择跟随快速R-CNN的7×7滤波器架构[10]，它为每个提案生成一个大小为7×7×512的特征映射。为了将每个提议的特征映射转换为特征向量，我们使用尺寸为7×7×512×2048的FC层。然后将与每个提议相关的2048维特征向量馈送到L2归一化层，随后点积图层，ReLU，我们的几何投票图层和丢失图层。卷积层由VGG16的预训练权重初始化，并且完全连接的层具有随机初始化。我们通过小批量SGD来培训我们的SCNet，学习率为0.001，重量衰减为0.0005。在培训期间，每个小批量产生自与一些建议相关的一对图像。在我们的实施中，我们为每幅图像生成了500个提案，从而导致500×500个潜在匹配。对于每个小批次，我们按照以下方式对训练进行比较。 （1）积极抽样：对于IA中的提案ri，我们在IB中给出它的地面实况匹配r（cid：48）i。我们选择IB中的所有提案，并将IoU（sj，r（cid：48）i）\x26gt



st in IB with I oU (st , r (cid:48) 

st在IB与I oU（st，r（cid：48）



satisfying the IoU constraint, we ﬁnd the proposals with top k appearance similarity with ri among those p proposals. In our experiment, we set Tpos = 0.6, and Tneg = 0.4. 

满足IoU约束条件，我们在这些建议中找到与ri具有最高k外观相似性的建议。在我们的实验中，我们设定Tpos \x3d 0.6，Tneg \x3d 0.4。



5. Experimental evaluation 

5.实验评估



In this section we present experimental results and analysis. Our code and models will be made available online: 

在本节中，我们将介绍实验结果和分析。我们的代码和模型将在网上提供：



http://www.di.ens.fr/willow/research/scnet/. 





5.1. Experimental details 

5.1。实验细节



Dataset. We use the PF-PASCAL dataset that consists of 1300 image pairs selected from PASCAL-Berkeley keypoint annotations5 of 20 object classes. Each pair of images in PF-PASCAL share the same set of non-occluded keypoints. We divide the dataset into 700 training pairs, 300 validation pairs, and 300 testing pairs. The image pairs for training/validation/testing are distributed proportionally to the number of image pairs of each object class. In training, we augment the data into a total of 1400 pairs by horizontal mirroring. We also test our trained models with the PF-WILLOW dataset [11], Caltech-101 [8] and PASCAL Parts [49] to further validate a generalization of the models. 

数据集。我们使用PF-PASCAL数据集，该数据集由从20个对象类的PASCAL-Berkeley关键点注释5中选择的1300个图像对组成。 PF-PASCAL中的每一对图像共享相同的一组非关闭点。我们将数据集划分为700个训练对，300个验证对和300个测试对。用于训练/验证/测试的图像对与每个对象类别的图像对的数量成比例地分配。在训练中，我们通过水平镜像将数据扩充为1400对。我们还用PF-WILLOW数据集[11]，Caltech-101 [8]和PASCAL Parts [49]测试我们的训练模型，以进一步验证模型的推广。



Region proposal. Unless stated otherwise, we choose to use the method of Manen et al. (RP) [29]. The use of RP proposals is motivated by the superior result reported in [11], which is veriﬁed once more by our evaluation. In testing we use 1000 proposals for each image as in [11], while in training we use 500 proposals for efﬁciency. 

地区建议。除非另有说明，否则我们选择使用Manen等人的方法。 （RP）[29]。 RP提案的使用受到[11]中报告的优异结果的启发，我们的评估再次验证了这一结果。在测试中，我们对每个图像使用1000个提议，如[11]，而在培训中我们使用500个提议来提高效率。



Evaluation metric. We use three metrics to compare the results of SCNet to other methods. First, we use the probability of correct keypoint (PCK) [44], which measures the precision of dense ﬂow at sparse keypoints of semantic relevance. It is calculated on the Euclidean distance d(φ(p), p∗ ) between a warped keypoint φ(p) and ground-truth one p∗ 6 . Second, we use the probability of correct regions (PCR) introduced in [11] as an equivalent of the the PCK for region based correspondence. PCR measures the precision of a region matching between region r and its correspondent r∗ on the intersection over union (IoU) score 1 − IoU(φ(r), r∗ ). Both metrics are computed against a threshold τ in [0, 1] and we measure PCK@τ and PCR@τ as the percentage correct below τ . Third, we capture the quality of matching proposals by the mean IoU of the top k matches (mIoU@k). Note that these metrics are used to evaluate two different types of correspondence. Indeed, PCK is an evaluation metric for dense ﬂow ﬁeld, whereas PCR and mIoU@k are used to evaluate region-based correspondences [11]. 

评估指标。我们使用三个指标来比较SCNet和其他方法的结果。首先，我们使用正确关键点（PCK）[44]的概率，它测量稀疏关键点语义关联密集流的精度。它是根据扭曲关键点φ（p）和地面真值之间的欧几里得距离d（φ（p），p *）来计算的。其次，我们使用[11]中引入的正确区域概率（PCR）作为基于区域的对应关系的PCK的等价值。 PCR测量区域r与其相应r *在联合（IoU）评分1-IoU（φ（r），r *）上的相交区域的匹配精度。两个指标都是根据[0，1]中的阈值τ来计算的，我们测量的是PCK @τ和PCR @τ作为低于τ的正确百分比。第三，我们通过前k个匹配的平均IoU（mIoU @ k）捕获匹配建议的质量。请注意，这些度量标准用于评估两种不同类型的通信。事实上，PCK是密集流场的评估指标，而PCR和mIoU k用于评估区域对应[11]。



5 http://www.di.ens.fr/willow/research/proposalﬂow/ 6 To better take into account the different sizes of images, we normalize the distance by dividing by the diagonal of the warped image, as in [5] 

为了更好地考虑不同大小的图像，我们用距离除以扭曲图像的对角线来规范距离，如[5]中所示，



(a) 

（一个）



(b) 

（b）中



(c) 

（C）



(d) 

（d）



Figure 3: (a) Performance of SCNet on PF-PASCAL, compared to Proposal Flow methods [11]. (b) Performance of SCNet and HOG descriptors on PF-PASCAL, evaluated using Proposal Flow methods [11]. (c) Comparison to ImageNet-trained baselines. (d) Comparison of different proposals. PCR and mIoU@k plots are shown at the top and bottom, respectively. AuC is shown in the legend. (Best viewed in pdf.) 

图3：（a）PF-PASCAL上SCNet的性能，与Proposal Flow方法[11]相比较。 （b）使用Proposal Flow方法对PF-PASCAL上SCNet和HOG描述符的性能进行评估[11]。 （c）与ImageNet训练的基线进行比较。 （d）比较不同的提案。 PCR和mIoU @ k图分别显示在顶部和底部。 AuC显示在图例中。 （以PDF格式查看最佳）



5.2. Proposal ﬂow components 

5.2。建议流程组件



We use the PF-PASCAL dataset to evaluate region matching performance. This setting allows our method to be tested against three other methods in [11]: NAM, PHM and LOM. NAM ﬁnds correspondences using handcrafted features only. PHM and LOM additionally consider global and local geometric consistency, respectively, between region matchings. We also compare our SCNet-learned feature against whitened HOG [6], the best performing handcraft feature of [11]. Experiments on the PF-WILLOW dataset [11] showed similar results with the ones on the PFPASCAL dataset. For details, refer to our project webpage. 

我们使用PF-PASCAL数据集来评估区域匹配性能。这个设置允许我们的方法在[11]中与其他三种方法进行测试：NAM，PHM和LOM。 NAM仅使用手工制作的功能找到对应关系。 PHM和LOM分别考虑区域匹配之间的全局和局部几何一致性。我们还将我们的SCNet学习功能与[11]中表现最佳的手工功能与白色HOG [6]进行比较。 PF-WILLOW数据集上的实验[11]显示了与PFPASCAL数据集上的结果类似的结果。有关详情，请参阅我们的项目网页。



Quantitative comparison. Figure 3(a) compares SCNet 

定量比较。图3（a）比较了SCNet



methods with the proposal ﬂow methods [11] on the PFPASCAL dataset. Our SCNet models outperform the other methods that use the HOG feature. Our geometric models (SCNet-AG, SCNet-AG+) substantially outperform the appearance-only model (SCNet-A), and SCNetAG+ slightly outperform SCNet-AG. This can also be seen from the area under curve (AuC) presented in the legend. This clearly show the effectiveness of deep learned features as well as geometric matching. In this comparison, we ﬁx the VGG16 layer and only learn the FC layers. In our experiment, we also learned all layers including VGG 16 and the FC layers in our model (fully ﬁnetuned), but the improvement over the partially learned model was marginal. Figure 3(b) shows the performance of NAM, PHM, LOM matching when replacing HOG feature with our learned feature in SCNet-A. We see that SCNet features greatly improves all the matching methods. Interestingly, LOM us

方法与PFPASCAL数据集上的提案流程方法[11]。我们的SCNet模型比其他使用HOG功能的方法更胜一筹。我们的几何模型（SCNet-AG，SCNet-AG +）明显优于仅外观模型（SCNet-A），并且SCNetAG +略优于SCNet-AG。这也可以从图例中呈现的曲线下面的区域（AuC）看出。这清楚地表明了深入学习功能以及几何匹配的有效性。在这个比较中，我们找到了VGG16层，只学习了FC层。在我们的实验中，我们还学习了包括VGG 16和我们模型中FC层在内的所有图层（完全网格化），但对部分学习模型的改进是微不足道的。图3（b）显示了用SCNet-A中的我们的学习功能代替HOG特征时NAM，PHM和LOM匹配的性能。我们看到SCNet功能极大地改进了所有的匹配方法。有趣的是，LOM我们



ing SCNet feature outperforms our best performing SCNet model, SCNet-AG+. However, the LOM method is more than 10 times slower than SCNet-AG+: on average the method takes 0.21s for SCNet-A feature extraction and 3.15s for the actual matching process, whereas our SCNetAG+ only takes 0.33s in total. Most of the time in LOM is spent in computing its geometric consistency term. We further evaluated three additional baselines using ImageNettrained VGG (see Figure 3(c)). Namely (i) VGG: We directly use the features from ImageNet-trained VGG, followed by ROI-pooling to make the features for each proposal of the same size (7 × 7 × 512). We then ﬂatten the features into vectors of dimension 175616. (ii) VGG-L2: We l2-normalize the ﬂattened feature of (i). (iii) VGG-L2FC: We perform a random projection from (ii) to a feature of dimension 2048 (the same dimension with SCNet, 12.25 times smaller than (i) and (ii)) by adding a randomly initialized FC layer on top of (ii). Note that this is exactly equivalent to SCNet-A without training on the target dataset. 

SCNet功能优于我们最佳性能的SCNet型号SCNet-AG +。但是，LOM方法比SCNet-AG +慢10倍以上：平均而言，该方法对于SCNet-A特征提取需要0.21秒，对于实际匹配过程需要3.15秒，而我们的SCNetAG +总共只需要0.33秒。 LOM中的大部分时间都用于计算其几何一致性项。我们使用ImageNettrained VGG进一步评估了三个额外的基线（参见图3（c））。即（i）VGG：我们直接使用来自ImageNet训练的VGG的特征，然后是ROI池以为每个相同大小（7×7×512）的提案制作特征。 （ii）VGG-L2：我们将（i）的浮动特征标准化。 （iii）VGG-L2FC：我们通过在顶部增加一个随机初始化的FC层，从（ii）向尺寸2048的特征（与SCNet相同的尺寸，比（i）和（ii）小12.25倍） （ii）。请注意，这与未对目标数据集进行训练的SCNet-A完全等效。



Results with different object proposals. SCNet can be 

结果与不同的对象建议。 SCNet可以



combined with any region proposal methods. In this experiment, we train and evaluate SCNet-AG+ on PF-PASCAL with four region proposal methods: randomized prim (RP) [29], selective search (SS) [41], random uniform sampling (US), and sliding window (SW). US and SW are extracted using the work of [16], and SW is similar to regular grid sampling used in other popular methods [20, 26, 33]. Figure 3(d) compares matching performance in PCR and mIoU@k when using the different proposals. RP performs best, and US performs worst with a large margin. This 

结合任何区域提案方法。在这个实验中，我们使用四个区域提议方法：随机化原始（RP）[29]，选择性搜索（SS）[41]，随机均匀采样（US）和滑动窗口（PF），在PF-PASCAL上训练和评估SCNet- SW）。 US和SW使用[16]的工作进行提取，SW与其他常用方法[20,26,33]中使用的常规网格采样类似。图3（d）比较了PCR和mIoU @ k在使用不同提议时的匹配性能。 RP表现最好，美国表现最差，幅度较大。这个



bike image pair 

自行车图像对



NAMHOG [37] 

NAMHOG [37]



SCNet-A [104] 

SCNet-A [104]



SCNet-AG+ [107] 

SCNet-AG + [107]



wine bottle image pair 

葡萄酒瓶图像对



NAMHOG [88] 

NAMHOG [88]



SCNet-A [177] 

SCNet-A [177]



SCNet-AG+ [180] 

SCNet-AG + [180]



Figure 4: Region matching examples. Numbers beside methods stand for numbers of correct matches. 

图4：区域匹配示例。方法旁边的数字代表正确匹配的数字。



Table 1: Per-class PCK on PF-PASCAL at τ = 0.1. For all methods using object proposals, we use 1000 RP proposals [29]. 

表1：在τ\x3d 0.1时在PF-PASCAL上的每类PCK。对于使用对象提议的所有方法，我们使用1000个RP提议[29]。



Method aero bike bird boat bottle bus car cat chair cow d.table dog horse moto person plant sheep sofa train tv mean NAMHOG [11] 72.9 73.6 31.5 52.2 37.9 71.7 71.6 34.7 26.7 48.7 28.3 34.0 50.5 61.9 26.7 51.7 66.9 48.2 47.8 59.0 52.5 PHMHOG [11] 78.3 76.8 48.5 46.7 45.9 72.5 72.1 47.9 49.0 84.0 37.2 46.5 51.3 72.7 38.4 53.6 67.2 50.9 60.0 63.4 60.3 LOMHOG [11] 73.3 74.4 54.4 50.9 49.6 73.8 72.9 63.6 46.1 79.8 42.5 48.0 68.3 66.3 42.1 62.1 65.2 57.1 64.4 58.0 62.5 UCN [5] 64.8 58.7 42.8 59.6 47.0 42.2 61.0 45.6 49.9 52.0 48.5 49.5 53.2 72.7 53.0 41.4 83.3 49.0 73.0 66.0 55.6 SCNet-A 67.6 72.9 69.3 59.7 74.5 72.7 73.2 59.5 51.4 78.2 39.4 50.1 67.0 62.1 69.3 68.5 78.2 63.3 57.7 59.8 66.3 SCNet-AG 83.9 81.4 70.6 62.5 60.6 81.3 81.2 59.5 53.1 81.2 62.0 58.7 65.5 73.3 51.2 58.3 60.0 69.3 61.5 80.0 69.7 

方法航空自行车鸟船瓶公共汽车猫椅牛d.table狗马moto人植物绵羊沙发火车电视平均NAMHOG [11] 72.9 73.6 31.5 52.2 37.9 71.7 71.6 34.7 26.7 48.7 28.3 34.0 50.5 61.9 26.7 51.7 66.9 48.2 47.8 59.0 52.5 PHMHOG [11] 78.3 76.8 48.5 46.7 45.9 72.5 72.1 47.9 49.0 84.0 37.2 46.5 51.3 72.7 38.4 53.6 67.2 50.9 60.0 63.4 60.3 LOMHOG [11] 73.3 74.4 54.4 50.9 49.6 73.8 72.9 63.6 46.1 79.8 42.5 48.0 68.3 66.3 42.1 62.1 65.2 57.1 64.4 58.0 62.5 UCN [5] 64.8 58.7 42.8 59.6 47.0 42.2 61.0 45.6 49.9 52.0 48.5 49.5 53.2 72.7 53.0 41.4 83.3 49.0 73.0 66.0 55.6 SCNet-A 67.6 72.9 69.3 59.7 74.5 72.7 73.2 59.5 51.4 78.2 39.4 50.1 67.0 62.1 69.3 68.5 78.2 63.3 57.7 59.8 66.3 SCNet- AG 83.9 81.4 70.6 62.5 60.6 81.3 81.2 59.5 53.1 81.2 62.0 58.7 65.5 73.3 51.2 58.3 60.0 69.3 61.5 80.0 69.7



SCNet-AG+ 85.5 84.4 66.3 70.8 57.4 82.7 82.3 71.6 54.3 95.8 55.2 59.5 68.6 75.0 56.3 60.4 60.0 73.7 66.5 76.7 72.2 

SCNet-AG + 85.5 84.4 66.3 70.8 57.4 82.7 82.3 71.6 54.3 95.8 55.2 59.5 68.6 75.0 56.3 60.4 60.0 73.7 66.5 76.7 72.2



shows that the region proposal process is an important factor for matching performance. 

显示区域建议过程是匹配性能的重要因素。



Qualitative comparison. Region matching results for NAM, SCNet-A, and SCNet-AG+ are shown in Figure 4. In this example, at the IoU threshold 0.5, the numbers of correct matches are shown for all methods. We can see that SCNet models perform signiﬁcantly better than NAM with HOG feature, and SCNet-A is outperformed by SCNetAG+ that learns a geometric consistency term. 

定性比较。 NAM，SCNet-A和SCNet-AG +的区域匹配结果如图4所示。在本例中，在IoU阈值0.5时，显示所有方法的正确匹配数量。我们可以看到，SCNet模型的执行效率明显优于具有HOG特征的NAM，而SCNet-A则优于SCNetAG +，它学习了几何一致性项。



5.3. Flow ﬁeld 

5.3。流场



Given a sparse region matching result and its corresponding scores, we generate dense semantic ﬂow using a densifying technique in [11]. In brief, we select out a region match with the highest score, and assign dense correspondences to the pixels within the matched regions by linear interpolation. This process is repeated without replacement of the region match until we assign dense correspondences to all pixels in the source image. The results are evaluated on PF-PASCAL dataset. To evaluate transferability performance of the models, we also test them on other datasets such as PF-WILLOW [11], Caltech-101 [8] and PASCAL Parts [49] datasets, and compare with state-of-the-art results on these datasets. In these cases direct comparison between learning-based methods may not be fair in the sense that they are trained on different datasets. 

给定一个稀疏区域匹配结果及其相应的分数，我们使用致密技术[11]产生密集的语义流。简而言之，我们选出一个最高得分的区域匹配，并通过线性插值为匹配区域内的像素分配密集的对应关系。重复此过程而不替换区域匹配，直到我们为源图像中的所有像素分配密集的对应关系。结果在PF-PASCAL数据集上进行评估。为了评估模型的转移性能，我们还对其他数据集进行了测试，如PF-WILLOW [11]，Caltech-101 [8]和PASCAL Parts [49]数据集，并与最新的结果进行比较这些数据集。在这些情况下，基于学习的方法之间的直接比较可能不公平，因为他们在不同的数据集上进行了训练。



Results on PF-PASCAL. We compare SCNet with Proposal Flow [11] and UCN [5] on the PF-PASCAL dataset, and summarize the result in Table 1. The UCN is retrained using the code provided by the authors on the PF-PASCAL dataset for fair comparison. Using the raw network of [5] trained on a different subset of PASCAL yields as expected lower performance, with a mean PCK of 36.0 as opposed to the 55.6 obtained for the retrained network. The three variants of SCNet do consistently better than UCN as well as all methods in [11], with a PCK of 66.3 or above. Among all the methods, SCNet-AG+ performs best with a PCK of 72.2. Figure 5 presents two examples of dense matching for PF-PASCAL. Ground truth are presented as circles and predicted keypoints are presented as crosses. We observe a better performance of SCNet-AG and SCNet-AG+. 

PF-PASCAL上的结果。我们将SCNet与PF-PASCAL数据集上的Proposal Flow [11]和UCN [5]进行比较，并将结果总结在表1中。使用作者提供的PF-PASCAL数据集中的代码进行再训练以进行公平比较。使用[5]的原始网络对不同的PASCAL子集进行训练，得到预期的较低性能，平均PCK为36.0，而对于重新训练网络的平均PCK为55.6。 SCNet的三个变种一直比UCN以及[11]中的所有方法都更好，PCK为66.3或更高。在所有的方法中，SCNet-AG +在72.2的PCK上表现最佳。图5给出了两个PF-PASCAL密集匹配的例子。地面事实呈现为圆形，预测关键点呈现为十字形。我们观察到SCNet-AG和SCNet-AG +的更好性能。



Results on PF-WILLOW. For evaluating transferability, we test (PF-PASCAL trained) SCNet and UCN on the PFWILLOW dataset [11] and compare the results with recent methods in Table 2 where PCK is averaged over all classes. The postﬁx ‘w/SF’ and ‘w/PF’ represent that matching is performed by SIFT Flow [26] and Proposal Flow [11], respectively. On this dataset where the data has a different distribution, SCNet-AG slightly outperforms the A and AG+ variants (PCK@0.05). We observe that all SCNet models signiﬁcantly outperform UCN, which is trained on the same dataset with the SCNet models, as well as other methods 

PF-WILLOW的结果。为了评估可转移性，我们在PFWILLOW数据集[11]上测试（PF-PASCAL训练的）SCNet和UCN，并将结果与​​表2中最近的方法进行比较，其中PCK在所有类别上进行平均。 SF\x26#39



Source 

资源



Target 

目标



NAMHOG 

NAMHOG



LOMHOG 

LOMHOG



SCNet-A 

SCNet-A



SCNet-AG+ 

SCNet-AG +



Figure 5: Quantitative comparison of dense correspondence. We show the keypoints of the target image in circles and the predicted keypoints of the source in crosses, with a vector that depicts the matching error. (Best viewed in pdf.) 

图5：密集通信的定量比较。我们展示了圆圈中目标图像的关键点以及十字架中源的预测关键点，其中描述了匹配错误的向量。 （以PDF格式查看最佳）



Table 2: Fixed-threshold PCK on PF-WILLOW. 

表2：PF-WILLOW上的固定阈值PCK。



Table 3: Results on Caltech-101. 

表3：Caltech-101的结果。



Table 4: Results on PASCAL Parts. 

表4：PASCAL零件的结果。



Method SIFT Flow [26] DAISY w/SF [43] DeepC w/SF [46] LIFT w/SF [45] VGG w/SF [36] FCSS w/SF [21] FCSS w/PF [21] LOMHOG [11] UCN[5] SCNet-A SCNet-AG SCNet-AG+ 

方法SIFT流[26] DAISY w / SF [43] DeepC w / SF [46] LIFT w / SF [45] VGG w / SF [36] FCSS w / SF [21] FCSS w / PF [21] LOMHOG [ 11] UCN [5] SCNet-A SCNet-AG SCNet-AG +



PCK@0.05 PCK@0.1 PCK@0.15 0.247 0.380 0.504 0.324 0.456 0.555 0.212 0.364 0.518 0.224 0.346 0.489 0.224 0.388 0.555 0.354 0.532 0.681 0.295 0.584 0.715 0.284 0.568 0.682 0.291 0.417 0.513 0.390 

PCK@0.05 PCK@0.1 PCK@0.15 0.247 0.380 0.504 0.324 0.456 0.555 0.212 0.364 0.518 0.224 0.346 0.489 0.224 0.388 0.555 0.354 0.532 0.681 0.295 0.584 0.715 0.284 0.568 0.682 0.291 0.417 0.513 0.390



0.725 

0.725



0.873 

0.873



0.394 

0.394



0.386 

0.386



0.721 0.704 

0.721 0.704



0.871 0.853 

0.871 0.853



Methods NAMHOG [11] PHMHOG [11] LOMHOG [11] DeepFlow [33] SIFT Flow [26] DSP [20] FCSS w/SF [21] FCSS w/PF [21] SCNet-A SCNet-AG SCNet-AG+ 

方法NAMHOG [11] PHMHOG [11] LOMHOG [11] DeepFlow [33] SIFT流[26] DSP [20] FCSS w / SF [21] FCSS w / PF [21] SCNet -A SCNet-AG SCNet-AG +



LT-ACC IoU LOC-ERR 0.70 0.44 0.39 0.75 0.48 0.31 0.78 0.50 0.26 0.74 0.40 0.34 0.75 0.48 0.32 0.77 0.47 0.35 0.80 0.50 

LT-ACC IoU LOC-ERR 0.70 0.44 0.39 0.75 0.48 0.31 0.78 0.50 0.26 0.74 0.40 0.34 0.75 0.48 0.32 0.77 0.47 0.35 0.80 0.50



0.21 

0.21



0.22 0.28 0.27 0.25 

0.22 0.28 0.27 0.25



0.83 

0.83



0.52 

0.52



0.78 0.78 0.79 

0.78 0.78 0.79



0.50 0.50 0.51 

0.50 0.50 0.51



Methods IoU PCK NAMHOG [11] 0.35 0.13 PHMHOG [11] 0.39 0.17 LOMHOG [11] 0.41 0.17 Congealing [24] 0.38 0.11 RASL [32] 0.39 0.16 CollectionFlow [19] 0.38 0.12 DSP [20] 0.39 0.17 FCSS w/SF [21] 0.44 0.28 FCSS w/PF [21] SCNet-A SCNet-AG SCNet-AG+ 

方法IoU PCK NAMHOG [11] 0.35 0.13 PHMHOG [11] 0.39 0.17 LOMHOG [11] 0.41 0.17凝固[24] 0.38 0.11 RASL [32] 0.39 0.16 CollectionFlow [19] 0.38 0.12 DSP [20] 0.39 0.17 FCSS w / 21] 0.44 0.28 FCSS w / PF [21] SCNet-A SCNet-AG SCNet-AG +



0.47 0.17 0.47 0.17 

0.47 0.17 0.47 0.17



0.46 0.29 

0.46 0.29



0.48 0.18 

0.48 0.18



using hand-crafted features [26, 43, 22] and learned features [35, 46, 12, 45, 36, 21]. 

使用手工制作的特征[26,43,22]和学习特征[35,46,12,45,36,21]。



Results on Caltech-101. We also evaluate our approach on the Caltech-101 dataset [8]. Following the experimental protocol in [20], we randomly select 15 pairs of images for each object class, and evaluate matching accuracy with three metrics: Label transfer accuracy (LT-ACC) [25], the IoU metric, and the localization error (LOC-ERR) of corresponding pixel positions. Table 3 shows that SCNet achieves comparable results with the state of the art. The best performer, FCSS [21], is trained on images from the same Caltech-101 dataset, while SCNet models are not. 

在Caltech-101上的结果。我们也评估了我们在Caltech-101数据集上的方法[8]。根据[20]中的实验协议，我们随机选择每对象类别的15对图像，并用三个度量评估匹配精度：标签传输精度（LT-ACC）[25]，IoU度量和定位误差LOC-ERR）的相应像素位置。表3显示SCNet与现有技术达到了可比较的结果。表现最佳的FCSS [21]是对来自同一个Caltech-101数据集的图像进行训练，而SCNet模型则不是。



Results on PASCAL Parts. Following [11], we use the dataset provided by [49] where the images are sampled from the PASCAL part dataset [3]. For this experiment, we measure the weighted IoU score between transferred segments and the ground truth, with weights determined by the pixel area of each part. To evaluate alignment accuracy, we measure the PCK metric (α = 0.05) using keypoint annotations 

关于PASCAL零件的结果。在[11]之后，我们使用由[49]提供的数据集，其中图像从PASCAL部分数据集[3]中采样。对于这个实验，我们测量转移段与地面实况之间的加权IoU得分，其中权重由每个部分的像素面积确定。为了评估对准精度，我们使用关键点注释来测量PCK度量（α\x3d 0.05）



for the PASCAL classes. Following [11] once again, we use selective search (SS) to generate proposals for SCNet in this experiment. The results are summarized in Table 4. SCNet models outperform all other results on the dataset in IoU, and SCNet-AG+ performs best among them. FCSS w/PF [21] performs better in PCK on this dataset. These results verify that SCNet models have successfully learned semantic correspondence. 

为PASCAL类。继[11]之后，我们再次使用选择性搜索（SS）为本实验中的SCNet生成提案。结果总结在表4中。SCNet模型优于IoU中数据集的所有其他结果，并且SCNet-AG +在其中表现最佳。 FCSS w / PF [21]在PCK上对这个数据集表现更好。这些结果验证了SCNet模型已经成功地学习了语义对应。



6. Conclusion 

六，结论



We have introduced a novel model for learning semantic correspondence, and proposed the corresponding CNN architecture that uses object proposals as matching primitives and learns matching in terms of appearance and geometry. The proposed method substantially outperforms both recent deep learning architectures and previous methods based on hand-crafted features. The result clearly demonstrates the effectiveness of learning geometric matching for semantic correspondence. In future work, we will explore better models and architectures to leverage geometric information. 

我们引入了一种新的学习语义对应模型，并提出了相应的CNN体​​系结构，它使用对象提议作为匹配基元，并在外观和几何学方面学习匹配。所提出的方法大大优于最近的深度学习架构和基于手工特征的先前方法。结果清楚地表明了语义对应学习几何匹配的有效性。在未来的工作中，我们将探索更好的模型和架​​构来利用几何信息。



Acknowledgments. This work was supported by the ERC grants VideoWorld and Allegro, the Institut Universitaire de France, the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIP) (No. 2017R1C1B2005584) as well as the MSIT (Ministry of Science and ICT), Korea, under the ICT Consilience Creative program (IITP-2017-R0346-16-1007). We gratefully acknowledge the support of NVIDIA Corporation with the donation of a Titan X Pascal GPU used for this research. We also thank JunYoung Gwak and Christopher B. Choy for their help in comparing with UCN. 

致谢。这项工作得到了ERC对韩国政府资助的VideoWorld和Allegro，法国大学法学院，韩国国家研究基金会（NRF）资助（第2017R1C1B2005584号）以及MSIT（科学部和信息通信技术（ICT）），韩国在ICT Consilience Creative计划下（IITP-2017-R0346-16-1007）。我们非常感谢NVIDIA公司在此次研究中捐赠的Titan X Pascal GPU的支持。我们也感谢JunYoung Gwak和Christopher B. Choy帮助我们与UCN进行比较。



References 

参考



[1] P. Arbelaez, J. Pont-Tuset, J. Barron, F. Marques, and J. Malik. Multiscale combinatorial grouping. In Proc. IEEE Conf. Comp. Vision Patt. Recog., 2014. 2 [2] H. Bristow, J. Valmadre, and S. Lucey. Dense semantic correspondence where every pixel is a classiﬁer. In Proc. Int. Conf. Comp. Vision, 2015. 1, 2 [3] X. Chen, R. Mottaghi, X. Liu, S. Fidler, R. Urtasun, et al. Detect what you can: Detecting and representing objects using holistic models and body parts. In Proc. IEEE Conf. Comp. Vision Patt. Recog., 2014. 8 [4] M. Cho, S. Kwak, C. Schmid, and J. Ponce. Unsupervised object discovery and localization in the wild: Part-based matching with bottom-up region proposals. In Proc. IEEE Conf. Comp. Vision Patt. Recog., 2015. 2, 3 [5] C. Choy, J. Gwak, S. Savarese, and M. Chandraker. Universal correspondence network. In Proc. Neural Info. Proc. Systems, 2016. 1, 2, 4, 5, 7, 8 [6] N. Dalal and B. Triggs. Histograms of oriented gradients for human detection. In Proc. IEEE Conf. Comp. Vision Patt. Recog., 2005. 1, 6 [7] M. Everingham, L. Van Gool, C. Williams, J. Winn, and A. Zisserman. The pascal visual object classes challenge 2007 (voc2007) results. 1 [8] L. Fei-Fei, R. Fergus, and P. Perona. One-shot learning of object categories. IEEE Trans. Patt. Anal. Mach. Intell., 28(4):594–611, 2006. 5, 7, 8 [9] P. Fischer, A. Dosovitskiy, E. Ilg, P. H ¨ausser, C. Hazırbas¸ , V. Golkov, P. van der Smagt, D. Cremers, and T. Brox. Flownet: Learning optical ﬂow with convolutional networks. In Proc. IEEE Conf. Comp. Vision Patt. Recog., 2015. 1, 2 [10] R. Girshick. Fast r-cnn. In Proc. Int. Conf. Comp. Vision, 2015. 4, 5 [11] B. Ham, M. Cho, C. Schmid, and J. Ponce. Proposal ﬂow. In Proc. IEEE Conf. Comp. Vision Patt. Recog., 2016. 1, 2, 3, 5, 6, 7, 8 [12] X. Han, T. Leung, Y. Jia, R. Sukthankar, and A. C. Berg. MatchNet: Unifying feature and metric learning for patchbased matching. In Proc. IEEE Conf. Comp. Vision Patt. Recog., 2015. 1, 2, 8 [13] B. Hariharan, J. Malik, and D. Ramanan. Discriminative decorrelation for clustering and classiﬁcation. In Proc. European Conf. Comp. Vision, pages 459–472. Springer, 2012. 2 

[1] P. Arbelaez，J. Pont-Tuset，J. Barron，F. Marques和J. Malik。多尺度组合分组。在Proc。 IEEE会议。比较。视觉Patt。 [2] H. Bristow，J. Valmadre和S. Lucey。每个像素都是分类器的密集语义对应。在Proc。诠释。 CONF。比较。 Vision，2015. 1，2 [3] X. Chen，R. Mottaghi，X. Liu，S. Fidler，R. Urtasun，et al。检测你可以做什么：使用整体模型和身体部位检测和表示对象。在Proc。 IEEE会议。比较。视觉Patt。 8 [4] M. Cho，S. Kwak，C. Schmid和J. Ponce。无人监管的对象发现和野外本地化：基于部分的匹配与自下而上的区域提案。在Proc。 IEEE会议。比较。视觉Patt。 Recog。，2015. 2，3 [5] C. Choy，J. Gwak，S. Savarese和M. Chandraker。通用通信网络。在Proc。神经信息。 PROC。系统，2016年。1，2，4，5，7，8 [6] N.达拉尔和B. Triggs。面向人体检测的梯度直方图。在Proc。 IEEE会议。比较。视觉Patt。 Recog。，2005. 1，6 [7] M. Everingham，L. Van Gool，C. Williams，J. Winn和A. Zisserman。帕斯卡视觉对象课挑战2007（voc2007）结果。 1 [8] L.菲菲，R. Fergus和P. Perona。对象类别的一次性学习。 IEEE Trans。帕特。肛门。马赫。 Intell。，28（4）：594-611,2006。5,7,8 [9] P.Fischer，A.Dosovitskiy，E.Ilg，P.H.ausser，C.Hazırbas，V.Golkov，P van der Smagt，D. Cremers和T. Brox。 Flownet：利用卷积网络学习光流。在Proc。 IEEE会议。比较。视觉Patt。 Recog。，2015. 1，2 [10] R. Girshick。快速r-cnn。在Proc。诠释。 CONF。比较。愿景，2015年。4，5 [11] B. Ham，M. Cho，C. Schmid和J. Ponce。建议流量。在Proc。 IEEE会议。比较。视觉Patt。 2016年。1,2,3,5,6,7,8 [12] X. Han，T. Leung，Y. Jia，R. Sukthankar和A. C. Berg。 MatchNet：为基于补丁的匹配统一功能和度量学习。在Proc。 IEEE会议。比较。视觉Patt。 Recog。，2015. 1,2,8 [13] B. Hariharan，J. Malik和D. Ramanan。用于聚类和分类的判别去相关。在Proc。欧洲会议。比较。愿景，第459-472页。斯普林格，2012。2



[14] K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling in deep convolutional networks for visual recognition. In Proc. European Conf. Comp. Vision, 2014. 4 [15] B. K. Horn and B. G. Schunck. Determining optical ﬂow: A retrospective. Artiﬁcial Intelligence, 59(1):81–87, 1993. 1 [16] J. Hosang, R. Benenson, P. Doll ´ar, and B. Schiele. What makes for effective detection proposals? IEEE Trans. Patt. Anal. Mach. Intell., 2015. 2, 6 [17] J. Hur, H. Lim, C. Park, and S. C. Ahn. Generalized deformable spatial pyramid: Geometry-preserving dense correspondence estimation. In Proc. IEEE Conf. Comp. Vision Patt. Recog., 2015. 1, 2 [18] A. Kanazawa, D. W. Jacobs, and M. Chandraker. WarpNet: Weakly supervised matching for single-view reconstruction. In Proc. IEEE Conf. Comp. Vision Patt. Recog., 2016. 2 [19] I. Kemelmacher-Shlizerman and S. M. Seitz. Collection ﬂow. In Proc. IEEE Conf. Comp. Vision Patt. Recog., 2012. 8 [20] J. Kim, C. Liu, F. Sha, and K. Grauman. Deformable spatial pyramid matching for fast dense correspondences. In Proc. IEEE Conf. Comp. Vision Patt. Recog., 2013. 1, 2, 6, 8 [21] S. Kim, D. Min, B. Ham, S. Jeon, S. Lin, and K. Sohn. Fcss: Fully convolutional self-similarity for dense semantic correspondence. In Proc. IEEE Conf. Comp. Vision Patt. Recog., 2017. 1, 2, 8 [22] S. W. Kim, D. Min, B. Ham, and K. Sohn. Dasc: Dense adaptative self-correlation descriptor for multi-modal and multispectral correspondence. In Proc. IEEE Conf. Comp. Vision Patt. Recog., 2015. 8 [23] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classiﬁcation with deep convolutional neural networks. In Proc. Neural Info. Proc. Systems, 2012. 2 [24] E. G. Learned-Miller. Data driven image models through continuous joint alignment. IEEE Trans. Patt. Anal. Mach. Intell., 28(2):236–250, 2006. 8 [25] C. Liu, J. Yuen, and A. Torralba. Nonparametric scene parsing via label transfer. IEEE Trans. Patt. Anal. Mach. Intell., 33(12):2368–2382, 2011. 8 [26] C. Liu, J. Yuen, and A. Torralba. SIFT ﬂow: Dense correspondence across scenes and its applications. IEEE Trans. Patt. Anal. Mach. Intell., 33(5):978–994, 2011. 1, 2, 6, 7, 8 [27] J. L. Long, N. Zhang, and T. Darrell. Do convnets learn correspondence? In Proc. Neural Info. Proc. Systems, 2014. 2 [28] D. G. Lowe. Distinctive image features from scale-invariant keypoints. Int. J. of Comp. Vision, 60(2):91–110, 2004. 1 [29] S. Manen, M. Guillaumin, and L. Van Gool. Prime object proposals with randomized Prim’s algorithm. In Proc. Int. Conf. Comp. Vision, 2013. 1, 2, 5, 6, 7 [30] J. Matas, O. Chum, M. Urban, and T. Pajdla. Robust widebaseline stereo from maximally stable extremal regions. Image and vision computing, 22(10):761–767, 2004. 1 [31] M. Okutomi and T. Kanade. A multiple-baseline stereo. IEEE Trans. Patt. Anal. Mach. Intell., 15(4):353–363, 1993. 1 [32] Y. Peng, A. Ganesh, J. Wright, W. Xu, and Y. Ma. Rasl: Robust alignment by sparse and low-rank decomposition for 

[14] K. He，X. Zhang，S. Ren和J. Sun.空间金字塔池在深度卷积网络中进行视觉识别。在Proc。欧洲会议。比较。愿景，2014年。4 [15] B. K. Horn和B. G. Schunck。确定光流量：回顾。人工智能，59（1）：81-87,1993。1 [16] J.Hosang，R.Benenson，P.Doll\x26#39



consistency. In Proc. IEEE Conf. Comp. Vision Patt. Recog., 2016. 1, 2 [51] C. L. Zitnick and P. Doll ´ar. Edge boxes: Locating object proposals from edges. In Proc. European Conf. Comp. Vision, 2014. 1, 2 

一致性。在Proc。 IEEE会议。比较。视觉Patt。 Recog。，2016。1，2 [51] C. L. Zitnick和P. Doll\x26#39



linearly correlated images. IEEE Trans. Patt. Anal. Mach. Intell., 34(11):2233–2246, 2012. 8 [33] J. Revaud, P. Weinzaepfel, Z. Harchaoui, and C. Schmid. Deepmatching: Hierarchical deformable dense matching. ArXiv e-prints, 2015. 1, 6, 8 [34] C. Rhemann, A. Hosni, M. Bleyer, C. Rother, and M. Gelautz. Fast cost-volume ﬁltering for visual correspondence and beyond. In Proc. IEEE Conf. Comp. Vision Patt. Recog., 2011. 1 [35] E. Simo-Serra, E. Trulls, L. Ferraz, I. Kokkinos, P. Fua, and F. Moreno-Noguer. Discriminative learning of deep convolutional feature point descriptors. In Proc. Int. Conf. Comp. Vision, 2015. 2, 8 [36] K. Simonyan and andrew Zisserman. Very deep convolutional networks for large-scale visual recognition. In Proc. IEEE Conf. Comp. Vision Patt. Recog., 2014. 5, 8 [37] T. Taniai, S. N. Sinha, and Y. Sato. Joint recovery of dense correspondence and cosegmentation in two images. In Proc. IEEE Conf. Comp. Vision Patt. Recog., 2016. 1, 2 [38] M. Tau and T. Hassner. Dense correspondences across scenes and scales. IEEE Trans. Patt. Anal. Mach. Intell., 2015. 2 [39] E. Tola, V. Lepetit, and P. Fua. Daisy: An efﬁcient dense descriptor applied to wide-baseline stereo. IEEE Trans. Patt. Anal. Mach. Intell., 32(5):815–830, 2010. 1, 2 [40] J. R. Uijlings, K. E. van de Sande, T. Gevers, and A. W. Smeulders. Selective search for object recognition. Int. J. of Comp. Vision, 104(2):154–171, 2013. 1, 2 [41] J. R. R. Uijlings, K. E. A. van de Sande, T. Gevers, and A. W. M. Smeulders. Selective search for object recognition. Int. J. of Comp. Vision, 104(2):154–171, 2013. 6 [42] P. Weinzaepfel, J. Revaud, Z. Harchaoui, and C. Schmid. Deepﬂow: Large displacement optical ﬂow with deep matching. In Proc. Int. Conf. Comp. Vision, 2013. 1 [43] H. Yang, W.-Y. Lin, and J. Lu. Daisy ﬁlter ﬂow: A generalized discrete approach to dense correspondences. In Proc. IEEE Conf. Comp. Vision Patt. Recog., 2014. 1, 2, 8 [44] Y. Yang and D. Ramanan. Articulated human detection with ﬂexible mixtures of parts. IEEE Trans. Patt. Anal. Mach. Intell., 35(12):2878–2890, 2013. 5 [45] K. M. Yi, E. Trulls, V. Lepetit, and P. Fua. Lift: Learned invariant feature transform. In Proc. European Conf. Comp. Vision, 2016. 8 [46] S. Zagoruyko and N. Komodakis. Learning to compare image patches via convolutional neural networks. In Proc. IEEE Conf. Comp. Vision Patt. Recog., 2015. 2, 8 [47] J. ˇZbontar and Y. LeCun. Computing the stereo matching cost with a convolutional neural network. In Proc. IEEE Conf. Comp. Vision Patt. Recog., 2015. 1, 2 [48] J. Zbontar and Y. LeCun. Stereo matching by training a convolutional neural network to compare image patches. Journal of Machine Learning Research, 17(1-32):2, 2016. 1, 2 [49] T. Zhou, Y. Jae Lee, S. X. Yu, and A. A. Efros. FlowWeb: Joint image set alignment by weaving consistent, pixel-wise correspondences. In Proc. IEEE Conf. Comp. Vision Patt. Recog., 2015. 5, 7, 8 [50] T. Zhou, P. Kr ¨ahenb ¨uhl, M. Aubry, Q. Huang, and A. A. Efros. Learning dense correspondence via 3d-guided cycle 

线性相关图像。 IEEE Trans。帕特。肛门。马赫。 Intell。，34（11）：2233-2246,2012。8 [33] J.Revaud，P.Weinzaepfel，Z.Hchachaoui和C.Schmid。 Deepmatching：分层可变形密集匹配。 ArXiv e-prints，2015。1,6,8 [34] C. Rhemann，A. Hosni，M. Bleyer，C. Rother和M. Gelautz。用于视觉对应和超越的快速成本过滤。在Proc。 IEEE会议。比较。视觉Patt。 [35] E. Simo-Serra，E. Trulls，L. Ferraz，I. Kokkinos，P. Fua和F. Moreno-Noguer。深卷积特征点描述符的判别式学习。在Proc。诠释。 CONF。比较。愿景，2015年。2,8 [36] K. Simonyan和安德鲁Zisserman。用于大规模视觉识别的非常深的卷积网络。在Proc。 IEEE会议。比较。视觉Patt。 [39] T. Taniai，S. N. Sinha和Y. Sato。在两幅图像中联合恢复密集的对应关系和cosegmentation。在Proc。 IEEE会议。比较。视觉Patt。 2016年1月2日[38] M. Tau和T. Hassner。跨场景和尺度的密集对应。 IEEE Trans。帕特。肛门。马赫。 Intell。，2015.2 [39] E. Tola，V. Lepetit和P. Fua。 Daisy：适用于宽基线立体声的有效密集描述符。 IEEE Trans。帕特。肛门。马赫。 Intell。，32（5）：815-830，2010。1，2 [40] J.R.Uijlings，K.E.van de Sande，T.Gevers和A.W.Smeulders。选择性搜索对象识别。诠释。 J. of Comp。 Vision，104（2）：154-171,2013。1,2 [41] J.R.R.Uijlings，K.E.vande Sande，T.Gevers和A.W.M.Smeulders。选择性搜索对象识别。诠释。 J. of Comp。愿景，104（2）：154-171,2013。[42] P. Weinzaepfel，J. Revaud，Z. Harchaoui和C. Schmid。深层流动：深层匹配的大位移光流。在Proc。诠释。 CONF。比较。 Vision，2013. 1 [43] H. Yang，W.-Y. Lin和J. Lu。菊花过滤器流程：对密集对应的广义离散方法。在Proc。 IEEE会议。比较。视觉Patt。 2014年，1，2，8 [44] Y.Yang和D.Ramanan。用灵活的零件混合物进行人工检测。 IEEE Trans。帕特。肛门。马赫。 [45] K. M. Yi，E. Trulls，V. Lepetit和P. Fua。提升：学习不变特征变换。在Proc。欧洲会议。比较。愿景，2016年。[8] S. Zagoruyko和N. Komodakis。学习通过卷积神经网络比较图像块。在Proc。 IEEE会议。比较。视觉Patt。 Recog。，2015. 2,8 [47] J. Zbontar和Y. LeCun。用卷积神经网络计算立体匹配成本。在Proc。 IEEE会议。比较。视觉Patt。 Recog。，2015. 1，2 [48] J. Zbontar和Y. LeCun。通过训练卷积神经网络来比较图像块来进行立体匹配。机器学习研究期刊，17（1-32）：2，2016. 1,2 [49] T. Zhou，Y. Jae Lee，S.X.Yu和A.A.Efros。 FlowWeb：通过编织一致的，逐像素对应的联合图像集对齐方式。在Proc。 IEEE会议。比较。视觉Patt。 [50] T. Zhou，P. Kr，ahenb-uhl，M. Aubry，Q. Huang和A. A. Efros。通过3D引导的循环学习密集的通信



