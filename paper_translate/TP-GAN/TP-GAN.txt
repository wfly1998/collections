7 1 0 2 
7 1 0 2

g u 
你

A 
一个

4 
4

] 
]

V 
V

C 
C

. 
。

s 
小号

c 
C

[ 
[

2 v 6 8 0 4 0 
2 v 6 8 0 4 0

. 
。

4 0 7 1 
4 0 7 1

: 
：

v 
v

i 
一世

X 
X

r 
[R

a 
一个

Beyond Face Rotation: Global and Local Perception GAN for Photorealistic and Identity Preserving Frontal View Synthesis 
超越面部轮换：用于逼真和身份保持正面视图合成的全局和局部感知GAN

Rui Huang1,2 ∗† Shu Zhang1,2,3 ∗ Tianyu Li1,2 Ran He1,2,3 1National Laboratory of Pattern Recognition, CASIA 2Center for Research on Intelligent Perception and Computing, CASIA 3University of Chinese Academy of Sciences, Beijing, China 
Rui Huang1,2 *†Shu Zhang 1,2,3 * Tianyu Li1,2 Ran He1,2,3 1国家模式识别实验室，CASIA 2智能感知与计算研究中心，CASIA 3中国科学院大学，北京

huangrui@cmu.edu, tianyu.lizard@gmail.com, {shu.zhang, rhe}@nlpr.ia.ac.cn 
huangrui@cmu.edu，tianyu.lizard @ gmail.com，{shu.zhang，rhe}@nlpr.ia.ac.cn

Abstract 
抽象

Photorealistic frontal view synthesis from a single face image has a wide range of applications in the ﬁeld of face recognition. Although data-driven deep learning methods have been proposed to address this problem by seeking solutions from ample face data, this problem is still challenging because it is intrinsically ill-posed. This paper proposes a Two-Pathway Generative Adversarial Network (TP-GAN) for photorealistic frontal view synthesis by simultaneously perceiving global structures and local details. Four landmark located patch networks are proposed to attend to local textures in addition to the commonly used global encoderdecoder network. Except for the novel architecture, we make this ill-posed problem well constrained by introducing a combination of adversarial loss, symmetry loss and identity preserving loss. The combined loss function leverages both frontal face distribution and pre-trained discriminative deep face models to guide an identity preserving inference of frontal views from proﬁles. Different from previous deep learning methods that mainly rely on intermediate features for recognition, our method directly leverages the synthesized identity preserving image for downstream tasks like face recognition and attribution estimation. Experimental results demonstrate that our method not only presents compelling perceptual results but also outperforms state-of-theart results on large pose face recognition. 
单面图像的照片级逼真正面视图合成在人脸识别领域具有广泛的应用。虽然已经提出数据驱动的深度学习方法通​​过从充足的面部数据中寻求解决方案来解决这个问题，但是这个问题仍然具有挑战性，因为它本质上是不适合的。本文提出了一种双向生成对抗网络（TP-GAN），用于通过同时感知全局结构和局部细节来进行逼真的正面视图合成。除了常用的全局编码器解码器网络之外，还提出了四个具有里程碑意义的贴片网络来处理局部纹理。除了新颖的架构之外，我们通过引入对抗性损失，对称性损失和身份保持损失的组合来很好地约束这个不适定问题。组合损失函数利用正面分布和预训练的判别性深层面部模型来指导身份保持对正面视图的推断。与以往主要依赖于中间特征识别的深度学习方法不同，我们的方法直接利用合成的身份保持图像进行人脸识别和归因估计等下游任务。实验结果表明，我们的方法不仅具有引人注目的感知结果，而且在大型姿势人脸识别方面也优于最先进的结果。

1. Introduction 
1.简介

Beneﬁting from the rapid development of deep learning methods and the easy access to a large amount of annotated face images, unconstrained face recognition techniques [31, 32] have made signiﬁcant advances in recent years. Although surpassing human performance has been 
从深度学习方法的快速发展和易于获取大量带注释的人脸图像中受益，无约束的人脸识别技术[31,32]近年来取得了重大进展。虽然超越了人类的表现

∗ These two authors contributed equally. †Homepage http://andrew.cmu.edu/user/ruih2/ 
*这两位作者的贡献相同。 †主页http://andrew.cmu.edu/user/ruih2/

Figure 1. Frontal view synthesis by TP-GAN. The upper half shows the 90◦ proﬁle image (middle) and its corresponding synthesized and ground truth frontal face. We invite the readers to guess which side is our synthesis results (please refer to Sec. 1 for the answer). The lower half shows the synthesized frontal view faces from proﬁles of 90◦ , 75◦ and 45◦ respectively. 
图1. TP-GAN的正面视图合成。上半部分显示90°专业图像（中间）及其相应的合成和地面真实正面。我们邀请读者猜测哪一方是我们的综合结果（请参阅第1节的答案）。下半部分分别显示了90°，75°和45°的合成正面视图。

achieved on several benchmark datasets [28], pose variations are still the bottleneck for many real-world application scenarios. Existing methods that address pose variations can be divided into two categories. One category tries to adopt hand-crafted or learned pose-invariant features [4,28], while the other resorts to synthesis techniques to recover a frontal view image from a large pose face image and then use the recovered face images for face recognition [45, 46]. For the ﬁrst category, traditional methods often make use of robust local descriptors such as Gabor [5], Haar [35] and LBP [2] to account for local distortions and then adopt metric learning [4, 36] techniques to achieve pose invariance. In contrast, deep learning methods often handle position variances with pooling operation and employ triplet loss [28] or 
在几个基准数据集上实现[28]，姿势变化仍然是许多实际应用场景的瓶颈。解决姿势变化的现有方法可以分为两类。一个类别试图采用手工制作或学习的姿势不变特征[4,28]，而另一个类别采用合成技术从大型姿势人脸图像中恢复正面视图图像，然后使用恢复的人脸图像进行人脸识别[ 45,46]。对于第一类，传统方法通常利用强大的局部描述符，如Gabor [5]，Haar [35]和LBP [2]来解释局部失真，然后采用度量学习[4,36]技术来实现姿态不变性。相比之下，深度学习方法通​​常处理与集合操作的位置差异并使用三元组损失[28]或

1 
1

contrastive loss [31] to ensure invariance to very large intraclass variations. However, due to the tradeoff between invariance and discriminability, these approaches cannot deal with large pose cases effectively. For the second category, earlier efforts on frontal view synthesis usually utilize 3D geometrical transformations to render a frontal view by ﬁrst aligning the 2D image with either a general [12] or an identity speciﬁc [32, 44] 3D model. These methods are good at normalizing small pose faces, but their performance decreases under large face poses due to severe texture loss. Recently, deep learning based methods are proposed to recover a frontal face in a data-driven way. For instance, Zhu et al. [46] propose to disentangle identity and pose representations while learning to estimate a frontal view. Although their results are encouraging, the synthesized image sometimes lacks ﬁne details and tends to be blurry under a large pose so that they only use the intermediate features for face recognition. The synthesized image is still not good enough to perform other facial analysis tasks, such as forensics and attribute estimation. Moreover, from an optimization point of view, recovering the frontal view from incompletely observed proﬁle is an ill-posed or under-deﬁned problem, and there exist multiple solutions to this problem if no prior knowledge or constraints are considered. Therefore, the quality of recovered results heavily relies on the prior or the constraints exploited in the training process. Previous work [17, 41, 45, 46] usually adopts pairwise supervision and seldom introduce constraints in the training process, so that they tend to produce blurry results. When human try to conduct a view synthesis process, we ﬁrstly infer the global structure (or a sketch) of a frontal face based on both our prior knowledge and the observed proﬁle. Then our attention moves to the local areas where all facial details will be ﬁlled out. Inspired by this process, we propose a deep architecture with two pathways (TP-GAN) for frontal view synthesis. These two pathways focus on the inference of global structure and the transformation of local texture respectively. Their corresponding feature maps are then fused for further process for the generation of the ﬁnal synthesis. We also make the recovery process well constrained by incorporating prior knowledge of the frontal faces’ distribution with a Generative Adversarial Network (GAN) [9]. The outstanding capacity of GAN in modeling 2D data distribution has signiﬁcantly advanced many ill-posed low level vision problems, such as superresolution [19] and inpainting [24]. Particularly, drawing inspiration from the faces’ symmetric structure, a symmetry loss is proposed to ﬁll out occluded parts. Moreover, to faithfully preserve the most prominent facial structure of an individual, we adopt a perceptual loss [16] in the compact feature space in addition to the pixel-wise L1 loss. Incorporating the identity preserving loss is critical for a faithful 
对比损失[31]，以确保非常大的类内变化的不变性。然而，由于在不变性和可辨性之间的权衡，这些方法不能有效地处理大的姿势情况。对于第二类，早期对正面视图合成的努力通常利用3D几何变换通过首先将2D图像与一般[12]或身份特定[32,44] 3D模型对齐来渲染正面视图。这些方法擅长对小姿势面进行归一化，但是由于严重的纹理损失，它们在大面部姿势下的性能会降低。最近，提出了基于深度学习的方法以数据驱动的方式恢复正面。例如，朱等人。 [46]建议在学习估计正面视图的同时解开身份和姿势表征。尽管它们的结果令人鼓舞，但合成图像有时缺乏细节，并且在大的姿势下往往模糊，因此它们仅使用中间特征进行面部识别。合成图像仍然不足以执行其他面部分析任务，例如取证和属性估计。此外，从优化的角度来看，从未完全观察到的概率中恢复正面视图是一个不适定或欠定义的问题，并且如果没有考虑先验知识或约束，则存在多个解决该问题的方案。因此，恢复结果的质量很大程度上依赖于训练过程中利用的先验或约束。以前的工作[17,17,45,46]通常采用成对监督，很少在训练过程中引入约束，因此它们往往会产生模糊的结果。当人类尝试进行视图合成过程时，我们首先根据我们的先验知识和观察到的概率来推断正面的全局结构（或草图）。然后我们的注意力转移到所有面部细节将被填充的当地区域。受此过程的启发，我们提出了一种具有两个路径（TP-GAN）的深层架构，用于正面视图合成。这两种途径分别关注全局结构的推断和局部纹理的转换。然后将它们相应的特征图融合，以进一步生成最终合成的过程。我们还通过将生成的对抗性网络（GAN）[9]纳入正面分布的先验知识，使恢复过程受到很好的约束。 GAN在2D数据分布建模方面的出色能力已经显着提升了许多不适定的低水平视觉问题，如超分辨[19]和修复[24]。特别是，从面部的对称结构中汲取灵感，提出了对称性损失来填充遮挡部分。此外，为了忠实地保留个体最突出的面部结构，除了像素方式的L1损失之外，我们在紧凑的特征空间中采用感知损失[16]。结合身份保护损失对忠实者来说至关重要

synthesis and greatly improves its potential to be applied to face analysis tasks. We show some samples generated by TP-GAN in the upper half of Fig. 1 (the left side of each tuple). The main contributions of our work lie in three folds: 1) We propose a human-like global and local aware GAN architecture for frontal view synthesis from a single image, which can synthesize photorealistic and identity preserving frontal view images even under a very large pose. 2) We combine prior knowledge from data distribution (adversarial training) and domain knowledge of faces (symmetry and identity preserving loss) to exactly recover the lost information inherent in projecting a 3D object into a 2D image space. 3) We demonstrate the possibility of a “recognition via generation” framework and outperform state-of-the-art recognition results under a large pose. Although some deep learning methods have been proposed for face synthesis, our method is the ﬁrst attempt to be effective for the recognition task with synthesized faces. 
合成并大大提高其应用于面部分析任务的潜力。我们在图1的上半部分（每个元组的左侧）显示了由TP-GAN生成的一些样本。我们工作的主要贡献在于三个方面：1）我们提出了一种类似人类的全局和局部感知GAN架构，用于从单个图像进行正面视图合成，即使在非常大的姿势下也可以合成照片级真实感和身份保持正面视图图像。 2）我们结合来自数据分布（对抗性训练）的先验知识和面部的领域知识（对称性和身份保持损失）来精确地恢复将3D对象投影到2D图像空间中所固有的丢失信息。 3）我们展示了“通过生成识别”框架的可能性，并且在大的姿势下超越了最先进的识别结果。虽然已经提出了一些面部合成的深度学习方法，但我们的方法是首次尝试对合成面部的识别任务有效。

2. Related Work 
2.相关工作

2.1. Frontal View Synthesis 
2.1。正面视图合成

Frontal view synthesis, or termed as face normalization, is a challenging task due to its ill-posed nature. Traditional methods address this problem either with 2D/3D local texture warping [12, 44] or statistical modeling [27]. For instance, Hassner et al. [12] employ a mean 3D model for face normalization. A joint frontal view synthesis and landmark localization method is proposed in [27] with a constrained low-rank minimization model. Recently, researchers employ Convolutional Neural Networks (CNN) for joint representation learning and view synthesis [17, 41, 45, 46]. Speciﬁcally, Yim et al. [41] propose a multi-task CNN to predict identity preserving rotated images. Zhu et al. [45, 46] develop novel architectures and learning objectives to disentangle the identity and pose representation while estimating the frontal view. Reed et al. [25] propose to use a Boltzmann machine to model factors of variation and generate rotated images via pose manifold traversal. Although it is much more convenient if the synthesized image can be directly used for facial analysis tasks, most of the previous methods mainly employ intermediate features for face recognition because they cannot faithfully produce an identity preserving synthesis. 
正面视图合成，或称为面部归一化，由于其不适合的性质，是一项具有挑战性的任务。传统方法通过2D / 3D局部纹理变形[12,44]或统计建模[27]解决了这个问题。例如，哈斯纳等人。 [12]采用平均3D模型进行面部归一化。在[27]中提出了一种联合正面视图合成和地标定位方法，其具有约束低秩最小化模型。最近，研究人员采用卷积神经网络（CNN）进行联合表示学习和视图合成[17,41,45,46]。具体而言，Yim等人。 [41]提出了一种多任务CNN来预测保持旋转图像的身份。朱等人。 [45,46]开发新颖的架构和学习目标，以在估计正面视图的同时解开身份和姿势表示。里德等人。 [25]建议使用玻尔兹曼机器模拟变异因子，并通过姿势流形遍历生成旋转图像。虽然如果合成图像可以直接用于面部分析任务更方便，但是大多数先前的方法主要采用中间特征进行面部识别，因为它们不能忠实地产生身份保持合成。

2.2. Generative Adversarial Network (GAN) 
2.2。生成对抗网络（GAN）

As one of the most signiﬁcant improvements on the research of deep generative models [18, 26], GAN [9] has drawn substantial attention from both the deep learning and computer vision society. The min-max two-player game provides a simple yet powerful way to estimate target distribution and generate novel image samples [6]. With its 
作为深度生成模型研究中最显着的改进之一[18,26]，GAN [9]引起了深度学习和计算机视觉社会的极大关注。 min-max双人游戏提供了一种简单而有效的方法来估计目标分布并生成新的图像样本[6]。随着它

2 
2

Figure 2. General framework of TP-GAN. The Generator contains two pathways with each processing global or local transformations. The Discriminator distinguishes between synthesized frontal (SF) views and ground-truth (GT) frontal views. Detailed network architectures can be found in the supplementary material. 
图2. TP-GAN的一般框架。 Generator包含两个路径，每个路径处理全局或局部变换。鉴别器区分合成正面（SF）视图和地面实况（GT）正面视图。详细的网络架构可以在补充材料中找到。

power for distribution modeling, the GAN can encourage the generated images to move towards the true image manifold and thus generates photorealistic images with plausible high frequency details. Recently, modiﬁed GAN architectures, conditional GAN [21] in particular, have been successfully applied to vision tasks like image inpainting [24], super-resolution [19], style transfer [20], face attribute manipulation [29] and even data augmentation for boosting classiﬁcation models [30,43]. These successful applications of GAN motivate us to develop frontal view synthesis methods based on GAN. 
用于分布建模的功率，GAN可以鼓励所生成的图像朝向真实图像流形移动，从而生成具有合理的高频细节的真实感图像。最近，修改过的GAN架构，尤其是条件GAN [21]已成功应用于视觉任务，如图像修复[24]，超分辨率[19]，样式转换[20]，面部属性操作[29]甚至数据增强分类模型的增强[30,43]。 GAN的这些成功应用激励我们开发基于GAN的正面视图合成方法。

3. Approach 
3.方法

The aim of frontal view synthesis is to recover a photorealistic and identity preserving frontal view image I F from a face image under a different pose, i.e. a proﬁle image I P . To train such a network, pairs of corresponding {I F , I P } from multiple identities y are required during the training phase. Both the input I P and output I F come from a pixel space of size W × H × C with C color channel. It’s our goal to learn a synthesis function that can infer the corresponding frontal view from any given proﬁle images. Speciﬁcally, we model the synthesis function with a two-pathway CNN GθG that is parametrized by θG . Each pathway contains an Encoder and a Decoder, denoted as } and {Gθl }, where g and l stand for the global structure pathway and the local texture pathway respectively. In the global pathway, the bottleneck layer, which is the output of Gθg , is usually used for classiﬁcation task [40] with the cross-entropy loss Lcross entropy . The network’s parameters GθG are optimized by mini
正面视图合成的目的是从不同姿势下的面部图像（即，轮廓图像I P）恢复照片写实和身份保持正面视图图像I F.为了训练这样的网络，在训练阶段期间需要来自多个身份y的相应{I F，I P}对。输入I P和输出I F都来自具有C颜色通道的尺寸为W×H×C的像素空间。我们的目标是学习一种综合功能，可以从任何给定的专业图像中推断出相应的正面视图。具体而言，我们使用由θG参数化的双路CNNGθG对合成函数进行建模。每个路径包含编码器和解码器，表示为}和{Gθl}，其中g和l分别代表全局结构路径和局部纹理路径。在全局路径中，作为Gθg输出的瓶颈层通常用于具有交叉熵损失Lcross熵的分类任务[40]。网络参数GθG由mini优化

{Gθg 
{Gθg

, Gθg 
，Gθg

D 
d

E 
Ë

, Gθl 
，Gθl

D 
d

E 
Ë

E 
Ë

N(cid:88) 
N（CID：88）

ˆθG = 
θG\x3d

1 N 
1 N.

argmin 
argmin

θG 
θG

{Lsyn (GθG (I P n ), I F n ) n ), yn )} (I P 
{Lsyn（GθG（I P n），I F n）n），yn）}（I P.

E 
Ë

n=1 +αLcross entropy (Gθg 
n \x3d 1 +αLcrossentropy（Gθg

(1) 
（1）

mizing a speciﬁcally designed synthesis loss Lsyn and the aforementioned Lcross entropy . For a training set with N training pairs of {I F n }, the optimization problem can be formulated as follows: 
模拟特定设计的合成损失Lsyn和上述Lcross熵。对于具有{I F n}的N个训练对的训练集，优化问题可以表述如下：

n , I P 
我，P

where α is a weighting parameter and Lsyn is deﬁned as a weighted sum of several losses that jointly constrain an image to reside in the desired manifold. We will postpone the detailed description of all the individual loss functions to Sec. 3.2. 
其中α是加权参数，Lsyn被定义为共同约束图像以驻留在所需流形中的若干损失的加权和。我们将把所有单个损失函数的详细描述推迟到Sec。 3.2。

3.1. Network Architecture 
3.1。网络架构

3.1.1 Two Pathway Generator 
3.1.1双路发生器

The general architecture of TP-GAN is shown in Fig. 2. Different from previous methods [17, 41, 45, 46] that usually model the synthesis function with one single network, our proposed generator GθG has two pathways, with one global network Gθg processing the global structure and four landmark located patch networks Gθl , i ∈ {0, 1, 2, 3} attending to local textures around four facial landmarks. We are not the ﬁrst to employ the two pathway modeling strategy. Actually, this is a quite popular routine for 2D/3D local texture warping [12, 44] methods. Similar to the human cognition process, they usually divide the normalization of faces into two steps, with the ﬁrst step to align the face globally with a 2D or 3D model and the second step to 
TP-GAN的一般体系结构如图2所示。与通常用单一网络模拟综合函数的先前方法[17,1,4,4,46]不同，我们提出的生成器GθG有两个路径，一个全局网络Gθg处理全局结构和四个地标定位的补丁网络Gθ1，i∈{0,1,2,3}，其遵循四个面部地标周围的局部纹理。我们不是第一个采用两种途径建模策略的人。实际上，对于2D / 3D局部纹理变形[12,44]方法来说，这是一个非常流行的例程。与人类认知过程类似，它们通常将面部的标准化分为两个步骤，第一步是将面部全局与2D或3D模型对齐，第二步是

i 
一世

3 
3

warp or render local texture to the global structure. Moreover, Mohammed et al. [22] combines a global parametric model with a local non-parametric model for novel face synthesis. Synthesizing a frontal face I F from a proﬁle image I P is a highly non-linear transformation. Since the ﬁlters are shared across all the spatial locations of the face image, we argue that using merely a global network cannot learn ﬁlters that are suitable for both rotating a face and precisely recovering local details. Therefore, we transfer the success of the two pathway structure in traditional methods to a deep learning based framework and introduce the humanlike two pathway generator for frontal view synthesis. As shown in Fig. 2, Gθg is composed of a downsampling Encoder Gθg and an up-sampling Decoder Gθg , extra skip layers are introduced for multi-scale feature fusion. The bottleneck layer in the middle outputs a 256dimension feature vector vid , which is used for identity classiﬁcation to allow for identity-preserving synthesis. At this bottleneck layer, as in [33], we concatenate a 100-dim Gaussian random noise to vid to model variations other than pose and identity. 
将局部纹理扭曲或渲染到全局结构。此外，穆罕默德等人。 [22]将全局参数模型与局部非参数模型相结合，用于新颖的人脸合成。从配置图像I P合成正面I F是高度非线性变换。由于过滤器在人脸图像的所有空间位置共享，我们认为仅使用全球网络无法学习适合旋转脸部和精确恢复局部细节的过滤器。因此，我们将传统方法中两种途径结构的成功转化为基于深度学习的框架，并引入用于正面视图合成的人类两路径生成器。如图2所示，Gθg由下采样编码器Gθg和上采样解码器Gθg组成，引入额外跳过层用于多尺度特征融合。中间的瓶颈层输出256维度特征向量vid，其用于身份分类以允许身份保持合成。在这个瓶颈层，如[33]，我们将100维高斯随机噪声连接到vid，以模拟姿势和身份以外的变化。

D 
d

E 
Ë

3.1.2 Landmark Located Patch Network 
3.1.2地标定位补丁网络

i 
一世

The four input patches of the landmark located patch network Gθl are center-cropped from four facial landmarks, i.e. left eye center, right eye center, nose tip and mouth cen, i ∈ {0, 1, 2, 3} learns a separate set of ﬁlters ter. Each Gθl for rotating the center-cropped patch to its corresponding frontal view (after rotation, the facial landmarks are still in the center). The architecture of the landmark located patch network is also based on an encoder-decoder structure, but it has no fully connected bottleneck layer. To effectively integrate the information from the global and local pathways, we adopt an intuitive method for feature map fusion. As shown in Fig. 2, we ﬁrstly fuse the output feature tensors (multiple feature maps) of four local pathways to one single feature tensor that is of the same spatial resolution as the global feature tensor. Speciﬁcally, we put each feature tensor at a “template landmark location”, and then a max-out fusing strategy is introduced to reduce the stitching artifacts on the overlapping areas. Then, we simply concatenate the feature tensor from each pathway to produce a fused feature tensor and then feed it to successive convolution layers to generate the ﬁnal synthesis output. 
位于贴片网络Gθ1的地标的四个输入贴片是从四个面部标志中心裁剪的，即左眼中心，右眼中心，鼻尖和嘴岑，i∈{0,1,2,3}学习单独的一组过滤器每个Gθ1用于将中心裁剪的贴片旋转到其相应的正面视图（旋转后，面部标志仍位于中心）。位于地标的补丁网络的架构也基于编码器 - 解码器结构，但它没有完全连接的瓶颈层。为了有效地整合来自全球和本地路径的信息，我们采用直观的方法进行特征地图融合。如图2所示，我们首先将四个局部路径的输出特征张量（多个特征图）融合到一个与全局特征张量具有相同空间分辨率的单个特征张量。具体而言，我们将每个特征张量放在“模板界标位置”，然后引入最大化融合策略以减少重叠区域上的拼接伪影。然后，我们简单地连接每个路径的特征张量以产生融合特征张量，然后将其馈送到连续的卷积层以生成最终的合成输出。

3.1.3 Adversarial Networks 
3.1.3对抗性网络

To incorporate prior knowledge of the frontal faces’ distribution into the training process, we further introduce an discriminator DθD to distinguish real frontal face images I F from synthesized frontal face images GθG (I P ), following the work of Goodfellow et al. [9]. We train DθD and GθG 
为了将正面分布的先验知识结合到训练过程中，我们进一步引入鉴别器DθD以根据Goodfellow等人的工作来区分真实正面图像I F与合成正面图像GθG（I P）。 [9]。我们训练DθD和GθG

in an alternating way to optimize the following min-max problem: 
以交替方式优化以下最小 - 最大问题：

EI F ∼P (I F ) log DθD (I F )+ min max EI P ∼P (I P ) log(1 − DθD (GθG (I P ))) 
EI F~P（I F）logDθD（I F）+ min max EI P~P（I P）log（1  - DθD（GθG（I P）））

θD 
θD

θG 
θG

(2) 
（2）

Solving this min-max problem will continually push the output of the generator to match the target distribution of the training frontal faces, thus it encourages the synthesized image to reside in the manifold of frontal faces, leading to photorealistic synthesis with appealing high frequency details. As in [30], our DθD outputs a 2 × 2 probability map instead of one scalar value. Each probability value now corresponds to a certain region instead of the whole face, and DθD can speciﬁcally focus on each semantic region. 
解决这个最小 - 最大问题将持续推动发生器的输出以匹配训练正面的目标分布，因此它促使合成图像驻留在正面的多个面中，从而导致具有吸引人的高频细节的照片级真实合成。如在[30]中，我们的DθD输出2×2概率图而不是一个标量值。现在每个概率值对应于某个区域而不是整个面部，并且DθD可以特定地集中在每个语义区域上。

3.2. Synthesis Loss Function 
3.2。合成损失函数

The synthesis loss function used in our work is a weighted sum of four individual loss functions, we will give a detailed description in the following sections. 
我们工作中使用的综合损失函数是四个单独损失函数的加权和，我们将在以下部分给出详细描述。

3.2.1 Pixel-wise Loss 
3.2.1像素损失

We adopt pixel-wise L1 loss at multiple locations to facilitate multi-scale image content consistency: 
我们在多个位置采用像素方式L1丢失，以促进多尺度图像内容的一致性：

Lpixel = 
Lpixel \x3d

1 W × H 
1 W×H

|I pred x,y − I gt x,y | 
|我预测x，y  - 我gt x，y |

(3) 
（3）

Speciﬁcally, the pixel wise loss is measured at the output of the global, the landmark located patch network and their ﬁnal fused output. To facilitate a deep supervision, we also add the constraint on multi-scale outputs of the Gθg . Although this loss will lead to overly smooth synthesis results, it is still an essential part for both accelerated optimization and superior performance. 
具体而言，像素明显的损失是在全球输出，位于地标的贴片网络及其最终融合输出处测量的。为了便于深入监督，我们还对Gθg的多尺度输出添加约束。虽然这种损失会导致合成结果过于平滑，但它仍然是加速优化和卓越性能的重要组成部分。

D 
d

3.2.2 Symmetry Loss 
3.2.2对称损失

Symmetry is an inherent feature of human faces. Exploiting this domain knowledge as a prior and imposing a symmetric constraint on the synthesized images may effectively alleviate the self-occlusion problem and thus greatly improve performance for large pose cases. Speciﬁcally, we deﬁne a symmetry loss in two spaces, i.e. the original pixel space and the Laplacian image space, which is robust to illumination changes. The symmetry loss of a face image takes the form: 
对称性是人脸的固有特征。利用该领域知识作为先验并对合成图像施加对称约束可以有效地减轻自遮挡问题，从而大大提高大型姿势情况的性能。具体而言，我们在两个空间中定义对称性损失，即原始像素空间和拉普拉斯图像空间，其对于光照变化是鲁棒的。面部图像的对称性损失采用以下形式：

W(cid:88) 
W（CID：88）

H(cid:88) 
H（CID：88）

x=1 
X \x3d 1

y=1 
Y \x3d 1

W/2(cid:88) x=1 
W / 2（cid：88）x \x3d 1

H(cid:88) 
H（CID：88）

y=1 
Y \x3d 1

Lsym = 
Lsym \x3d

1 W/2 × H 
1 W / 2×H

|I pred x,y − I pred W −(x−1),y | 
|我预测x，y  - 我预测W  - （x-1），y |

(4) 
（4）

For simplicity, we selectively ﬂip the input so that the occluded part are all on the right side. Besides, only the occluded part (right side) of I pred receives the symmetry 
为简单起见，我们选择性地输入输入，使得被遮挡的部分都在右侧。此外，只有I pred的遮挡部分（右侧）才能获得对称性

4 
4

(a) Proﬁle 
（a）专业文件

(b) Ours 
（b）我们的

(c) [33] 
（c）[33]

(d) [41] 
（d）[41]

(e) [8] 
（e）[8]

(f) [44] 
（f）[44]

(g) [12] 
（g）[12]

(h) Frontal 
（h）正面

Figure 3. Comparison with state-of-the-art synthesis methods under the pose of 45◦ (ﬁrst two rows) and 30◦ (last row). 
图3.在45°（前两行）和30°（最后一行）的姿势下与最先进的合成方法进行比较。

loss, i.e. we explicitly pull the right side to be closer to the left. Lsym ’s contribution is twofold, generating realistic images by encouraging a symmetrical structure and accelerating the convergence of TP-GAN by providing additional back-propagation gradient to relieve self-occlusion for extreme poses. However, due to illumination changes or intrinsic texture difference, pixel values are not strictly symmetric most of the time. Fortunately, the pixel difference inside a local area is consistent, and the gradients of a point along all directions are largely reserved under different illuminations. Therefore, the Laplacian space is more robust to illumination changes and more indicative for face structure. 
损失，即我们明确地将右侧拉近左侧。 Lsym的贡献是双重的，通过鼓励对称结构产生逼真的图像，并通过提供额外的反向传播梯度来加速TP-GAN的收敛，从而减轻极端姿势的自我遮挡。然而，由于光照变化或固有纹理差异，像素值在大多数时间内不是严格对称的。幸运的是，局部区域内的像素差异是一致的，并且沿着所有方向的点的梯度在很大程度上保留在不同的照明下。因此，拉普拉斯空间对于光照变化更稳健并且更能指示面部结构。

3.2.3 Adversarial Loss 
3.2.3对抗性损失

The loss for distinguishing real frontal face images I F from synthesized frontal face images GθG (I P ) is calculated as follows: 
用于从合成正面图像GθG（I P）区分真实正面图像I F的损失计算如下：

Ladv = 
Ladv \x3d

1 N 
1 N.

− log DθD (GθG (I P n )) 
 - 记录DθD（GθG（I P n））

(5) 
（5）

Ladv serves as a supervision to push the synthesized image to reside in the manifold of frontal view images. It can prevent blur effect and produce visually pleasing results. 
Ladv用作监督以推动合成图像驻留在正面视图图像的流形中。它可以防止模糊效果并产生视觉上令人愉悦的结果。

N(cid:88) 
N（CID：88）

n=1 
n \x3d 1的

where Wi , Hi denotes the spatial dimension of the last ith layer. The identity preserving loss enforces the prediction to have a small distance with the ground-truth in the compact deep feature space. Since the Light CNN is pre-trained to classify tens of thousands of identities, it can capture the most prominent feature or face structure for identity discrimination. Therefore, it is totally viable to leverage this loss to enforce an identity preserving frontal view synthesis. Lip has better performance when used with Ladv . Using Lip alone makes the results prone to annoying artifacts, because the search for a local minimum of Lip may go through a path that resides outside the manifold of natural face images. Using Ladv and Lip together can ensure that the search resides in that manifold and produces photorealistic image. 
其中Wi，Hi表示最后一个第i层的空间维度。身份保持损失强制预测与紧凑深度特征空间中的地面实况具有小的距离。由于Light CNN经过预先训练以对成千上万的身份进行分类，因此它可以捕获身份歧视的最突出的特征或面部结构。因此，利用这种损失来强制执行保持正面视图合成的身份是完全可行的。与Ladv一起使用时，Lip具有更好的性能。单独使用Lip会使结果容易出现恼人的伪影，因为搜索局部最小的Lip可能会经过一条位于自然面部图像之外的路径。同时使用Ladv和Lip可确保搜索位于该流形中并产生逼真的图像。

3.2.5 Overall Objective Function 
3.2.5总体目标函数

The ﬁnal synthesis loss function is a weighted sum of all the losses deﬁned above: 
最终的综合损失函数是上面定义的所有损失的加权和：

Lsyn = Lpixel + λ1Lsym + λ2Ladv + λ3Lip + λ4Ltv 
Lsyn \x3d Lpixel +λ1Lsym+λ2Ladv+λ3Lip+λ4Ltv

(7) We also impose a total variation regularization Ltv [16] on the synthesized result to reduce spike artifacts. 
（7）我们还对合成结果施加了总变差正则化Ltv [16]，以减少尖峰伪像。

3.2.4 Identity Preserving Loss 
3.2.4身份保护损失

4. Experiments 
4.实验

Preserving the identity while synthesizing the frontal view image is the most critical part in developing the “recognition via generation” framework. In this work, we exploit the perceptual loss [16] that is originally proposed for maintaining perceptual similarity to help our model gain the identity preserving ability. Speciﬁcally, we deﬁne the identity preserving loss based on the activations of the last two layers of the Light CNN [38]: 
在合成正面视图图像时保留身份是开发“通过生成识别”框架中最关键的部分。在这项工作中，我们利用最初提出的感知损失[16]来保持感知相似性，以帮助我们的模型获得身份保持能力。具体而言，我们根据Light CNN最后两层的激活来定义身份保持损失[38]：

2(cid:88) 
2（CID：88）

i=1 
I \x3d 1

Lip = 
唇\x3d

1 Wi × Hi 
1 Wi×Hi

Wi(cid:88) x=1 
Wi（cid：88）x \x3d 1

Hi(cid:88) y=1 
嗨（cid：88）y \x3d 1

|F (I P )i x,y − F (G(I pred ))i 
| F（I P）i x，y  -  F（G（I pred））i

x,y | 
x，y |

(6) 
（6）

5 
五

Except for synthesizing natural looking frontal view images, the proposed TP-GAN also aims to generate identity preserving image for accurate face analysis with off-theshelf deep features. Therefore, in this section, we demonstrate the merits of our model on qualitative synthesis results and quantitive recognition results in Sec. 4.1 and 4.2. Sec. 6.3 presents visualization of the ﬁnal deep feature representations to illustrate the effectiveness of TP-GAN. Finally, in Sec. 4.4, we conduct detailed algorithmic evaluation to demonstrate the advantages of the proposed twopathway architecture and synthesis loss function. Implementation details We use colorful images of size 128 × 128 × 3 in all our experiments for both the input 
除了合成自然的正面视图图像外，所提出的TP-GAN还旨在生成身份保持图像，以便使用现成的深层特征进行精确的面部分析。因此，在本节中，我们证明了我们的模型在定性合成结果和定量识别结果中的优点。 4.1和4.2。秒。 6.3展示了最终深度特征表示的可视化，以说明TP-GAN的有效性。最后，在Sec。 4.4，我们进行详细的算法评估，以证明所提出的twopathway架构和综合损失函数的优点。实施细节我们在所有实验中都使用尺寸为128×128×3的彩色图像进行输入

Figure 4. Synthesis results by TP-GAN under different poses. From left to right, the poses are 90◦ , 75◦ , 60◦ , 45◦ , 30◦ and 15◦ . The ground truth frontal images are provided at the last column. 
图4. TP-GAN在不同姿势下的合成结果。从左到右，姿势分别为90°，75°，60°，45°，30°和15°。地面实况正面图像在最后一列提供。

Figure 5. Challenging situations. The facial attributes, e.g. beard, eyeglasses are preserved by TP-GAN. The occluded forehead and cheek are recovered. 
图5.具有挑战性的情况。面部属性，例如胡须，眼镜由TP-GAN保存。被遮挡的额头和脸颊被收回。

Table 1. Rank-1 recognition rates (%) across views and illuminations under Setting 1. For all the remaining tables, only methods marked with * follow the “recognition via generation” procedure while others leverage intermediate features for face recognition. 
表1.设置1下的视图和照明的Rank-1识别率（％）。对于所有剩余的表，只有标有*的方法遵循“通过生成识别”程序，而其他表格利用中间特征进行人脸识别。

Method CPF [41] Hassner et al. * [12] HPN [7] FIP 40 [45] c-CNN Forest [39] Light CNN [38] TP-GAN* 
方法CPF [41] Hassner等。 * [12] HPN [7] FIP 40 [45] c-CNN森林[39]光CNN [38] TP-GAN *

±90◦ 29.82 31.37 47.26 9.00 
±90°29.82 31.37 47.26 9.00

64.03 
64.03

±75◦ ±60◦ ±45◦ ±30◦ ±15◦ 71.65 81.05 89.45 44.81 74.68 89.59 96.78 47.57 61.24 72.77 78.26 84.23 49.10 69.75 85.54 92.98 96.30 60.66 74.38 89.02 94.05 96.97 32.35 73.30 97.45 99.80 99.78 
±75°±60°±45°±30°±15°71.65 81.05 89.45 44.81 74.68 89.59 96.78 47.57 61.24 72.77 78.26 84.23 49.10 69.75 85.54 92.98 96.30 60.66 74.38 89.02 94.05 96.97 32.35 73.30 97.45 99.80 99.78

99.85 
99.85

99.78 
99.78

84.10 
84.10

92.93 
92.93

98.58 
98.58

I P and the prediction I pred = GθG (I P ). Our method is evaluated on MultiPIE [10], a large dataset with 750, 000+ images for face recognition under pose, illumination and expression changes. The feature extraction network, Light CNN, is trained on MS-Celeb-1M [11] and ﬁne-tuned on the original images of MultiPIE. Our network is implemented with Tensorﬂow [1]. The training of TP-GAN lasts for one day with a batch size of 10 and a learning rate of 10−4 . In all our experiments, we empirically set 
I P和预测I pred \x3dGθG（I P）。我们的方法在MultiPIE [10]上进行了评估，这是一个拥有750,000多个图像的大型数据集，用于在姿势，光照和表情变化下进行人脸识别。功能提取网络Light CNN在MS-Celeb-1M [11]上进行了训练，并在MultiPIE的原始图像上进行了调整。我们的网络是用Tensor fl ow [1]实现的。 TP-GAN的培训持续一天，批量为10，学习率为10-4。在我们所有的实验中，我们根据经验设定

α = 10−3 , λ1 = 0.3, λ2 = 10−3 , λ3 = 3 × 10−3 and λ4 = 10−4 . 
α\x3d 10-3，λ1\x3d 0.3，λ2\x3d 10-3，λ3\x3d 3×10-3且λ4\x3d 10-4。

4.1. Face Synthesis 
4.1。面部合成

Most of the previous work on frontal view synthesis are dedicated to address that problem within a pose range of ±60◦ . Because it is commonly believed that with a pose larger than 60◦ , it is difﬁcult to faithfully recover a frontal view image. However, we will show that given enough training data and a proper architecture and loss design, it is in fact feasible to recover photorealistic frontal views from very large poses. Fig. 4 shows TP-GAN’s ability to recover compelling identity-preserving frontal faces from any pose 
以前关于正面视图合成的大部分工作都致力于在±60°的姿势范围内解决该问题。因为通常认为姿势大于60°，很难忠实地恢复正面视图图像。但是，我们将展示给定足够的训练数据和适当的架构和损失设计，实际上可以从非常大的姿势恢复逼真的正面视图。图4显示了TP-GAN从任何姿势中恢复令人信服的保持身份的正面的能力

6 
6

(a) Ours (b) [41] 
（a）我们（b）[41]

(c) [8] 
（c）[8]

(d) [44] 
（d）[44]

(e) [12] 
（e）[12]

Figure 6. Mean faces from six images (within ±45◦ ) per identity. 
图6.每个标识的六个图像（±45°内）的平均面。

and Fig. 3 illustrates a comparison with state-of-the-art face frontalization methods. Note that most of TP-GAN’s competitors cannot deal with poses larger than 45◦ , therefore, we only report their results under 30◦ and 45◦ . Compared to competing methods, TP-GAN presents a good identity preserving quality while producing photorealistic synthesis. Thanks to the data-driven modeling with prior knowledge from Ladv and Lip , not only the overall face structure but also the occluded ears, cheeks and forehead can be hallucinated in an identity consistent way. Moreover, it also perfectly preserves observed face attributes in the original proﬁle image, e.g. eyeglasses and hair style, as shown in Fig. 5. To further demonstrate the stable geometry shape of the syntheses across multiple poses, we show the mean image of synthesized faces from different poses in Fig. 6. The mean faces from TP-GAN preserve more texture detail and contain less blur effect, showing a stable geometry shape across multiple syntheses. Note that our method does not rely on any 3D knowledge for geometry shape estimation, the inference is made through sheer data-driven learning. As a demonstration of our model’s superior generalization ability to in the wild faces, we use images from LFW [14] dataset to test a TP-GAN model trained solely on Multi-PIE. As shown in Fig. 7, although the resultant color tone is similar to images from Multi-PIE, TP-GAN can faithfully synthesize frontal view images with both ﬁner details and better global shapes for faces in LFW dataset compared to state-of-the-art methods like [12, 44]. 
图3示出了与现有技术的脸部正面化方法的比较。请注意，大多数TP-GAN，竞争对手都无法处理大于45的姿势，因此，我们只在30，¶和45下报告他们的结果。与竞争方法相比，TP-GAN在产生照片般逼真的合成的同时提供了良好的身份保持质量。得益于Ladv和Lip的先前知识的数据驱动建模，不仅整个面部结构而且闭塞的耳朵，脸颊和前额可以以一致的方式产生幻觉。此外，它还完美地保留了原始图像中观察到的面部属性，例如，眼镜和发型，如图5所示。为了进一步证明多个姿势的合成的稳定几何形状，我们在图6中显示了来自不同姿势的合成面的平均图像。来自TP-GAN的平均面保留更多纹理细节并包含更少的模糊效果，在多个合成中显示稳定的几何形状。请注意，我们的方法不依赖于任何3D知识进行几何形状估计，推理是通过纯粹的数据驱动学习来完成的。作为我们模型的演示，我们使用LFW [14]数据集中的图像来测试仅在Multi-PIE上训练的TP-GAN模型。如图7所示，虽然得到的色调与来自Multi-PIE的图像相似，但TP-GAN可以忠实地合成具有Ô¨Åner细节的正面视图图像和LFW数据集中面部的更好的全局形状。最先进的方法，如[12,44]。

Table 2. Rank-1 recognition rates (%) across views, illuminations and sessions under Setting 2. 
表2.设置2下的视图，照明和会话的Rank-1识别率（％）。

Method FIP+LDA [45] MVP+LDA [46] CPF [41] DR-GAN [33] Light CNN [38] TP-GAN* 
方法FIP + LDA [45] MVP + LDA [46] CPF [41] DR-GAN [33]光CNN [38] TP-GAN *

±90◦ ±75◦ ±60◦ ±45◦ ±30◦ 45.9 64.1 80.7 60.1 72.9 83.7 61.9 79.9 88.5 83.2 86.2 90.1 5.51 24.18 62.09 92.13 97.38 
±90°±75°±60°±45°±30°45.9 64.1 80.7 60.1 72.9 83.7 61.9 79.9 88.5 83.2 86.2 90.1 5.51 24.18 62.09 92.13 97.38

64.64 
64.64

77.43 
77.43

87.72 
87.72

95.38 
95.38

98.06 
98.06

±15◦ 90.7 92.8 95.0 94.0 98.59 
±15°90.7 92.8 95.0 94.0 98.59

98.68 
98.68

(a) LFW (b) Ours (c) [44] 
（a）LFW（b）我们的（c）[44]

(d) [12] 
（d）[12]

Figure 7. Synthesis results on the LFW dataset. Note that TP-GAN is trained on Mulit-PIE. 
图7. LFW数据集的综合结果。注意TP-GAN是在Mulit-PIE上训练的。

4.2. Identity Preserving Property 
4.2。身份保护财产

Face Recognition To quantitatively demonstrate our method’s identity preserving ability, we conduct face recognition on MultiPIE with two different settings. The experiments are conducted by ﬁrstly extracting deep features with Light-CNN [38] and then compare Rank-1 recognition accuracy with a cosine-distance metric. The results on the proﬁle images I P serve as our baseline and are marked by the notation Light-CNN in all tables. It should be noted that although many deep learning methods have been proposed for frontal view synthesis, none of their synthesized images proved to be effective for recognition tasks. In a recent study on face hallucination [37], the authors show that directly using a CNN synthesized high resolution face image for recognition will certainly degenerate the performance instead of improving it. Therefore, it is of great signiﬁcance to validate whether our synthesis results can boost the recognition performance (whether the “recognition via generation” procedure works). In Setting 1, we follow the protocol from [39], and only images from session one are used. We include images with neutral expression under 20 illuminations and 11 poses within ±90◦ . One gallery image with frontal view and illumination is used for each testing subject. There is no overlap between training and testing sets. Table 1 shows our recognition performance and the comparison with the stateof-the-art. TP-GAN consistently achieves the best performance across all angles, and the larger the angle, the greater the improvement. When compared with c-CNN Forest [39], which is an ensemble of three models, we achieve a performance boost of about 20% on large pose cases. In Setting 2, we follow the protocol from [41], where neural expression images from all four sessions are used. One gallery image is selected for each testing identity from their ﬁrst appearance. All synthesized images of MultiPIE 
人脸识别为了定量地展示我们方法的身份保持能力，我们使用两种不同的设置在MultiPIE上进行人脸识别。通过首先用Light-CNN [38]提取深度特征进行实验，然后将Rank-1识别精度与余弦距离度量进行比较。在配置图像I P上的结果作为我们的基线，并在所有表格中用符号Light-CNN标记。应该注意的是，尽管已经提出了许多用于正面视图合成的深度学习方法，但是它们的合成图像都没有被证明对于识别任务是有效的。在最近一项关于面部幻觉的研究[37]中，作者表明，直接使用CNN合成的高分辨率人脸图像进行识别肯定会使性能退化，而不是改善性能。因此，验证我们的综合结果是否可以提高识别性能（“通过生成识别”程序是否有效）具有重要意义。在设置1中，我们遵循[39]中的协议，并且仅使用来自会话1的图像。我们包括20个照明下的中性表达和±90°内11个姿势的图像。每个测试对象使用一个具有正面视图和照明的图库图像。训练集和测试集之间没有重叠。表1显示了我们的识别性能以及与现有技术的比较。 TP-GAN始终在所有角度都达到最佳性能，角度越大，改进越大。与c-CNN Forest [39]（三个模型的集合）相比，我们在大型姿势案例中实现了约20％的性能提升。在设置2中，我们遵循[41]中的协议，其中使用来自所有四个会话的神经表达图像。从每个测试标识的第一次出现中选择一个图库图像。 MultiPIE的所有合成图像

Table 3. Gender classiﬁcation accuracy (%) across views and illuminations. 
表3.不同视图和照明的性别分类准确度（％）。

Method 
方法

I P 
我P

60 
60

CPI* [41] Amir et al. * [8] 
CPI * [41] Amir等。 * [8]

I P 
我P

128 
128

Hassner et al. * [12] TP-GAN* 
哈斯纳等人。 * [12] TP-GAN *

±45◦ ±30◦ ±15◦ 85.46 87.14 90.05 76.80 78.75 81.55 77.65 79.70 82.05 86.22 87.70 90.46 83.83 84.74 87.15 
±45°±30°±15°85.46 87.14 90.05 76.80 78.75 81.55 77.65 79.70 82.05 86.22 87.70 90.46 83.83 84.74 87.15

90.71 
90.71

89.90 
89.90

91.22 
91.22

in this paper are from the testing identities under Setting 2. The result is shown in Table 2. Note that all the compared CNN based methods achieve their best performances with learned intermediate features, whereas we directly use the synthesized images following a “recognition via generation” procedure. Gender Classiﬁcation To further demonstrate the potential of our synthesized images on other facial analysis tasks, we conduct an experiment on gender classiﬁcation. All the compared methods in this part also follow the “recognition via generation” procedure, where we directly use their synthesis results for gender classiﬁcation. The CNN for gender classiﬁcation is of the same structure as the encoder Gθg and is trained on batch1 of the UMD [3] dataset. We report the testing performance on Multi-PIE (Setting-1) in Table 3. For fair comparison, we present the results on the unrotated original images in two resolutions, 128 × 128 (I P 60 ) respectively. TPGAN’s synthesis achieves a better classiﬁcation accuracy than the original proﬁle images due to normalized views. It’s not surprising to see that all other compared models perform worse than the baseline, as their architectures are not designed for the gender classiﬁcation task. Similar phenomenon is observed in [37] where synthesized high resolution face images severely degenerate the recognition performance instead of improving it. That indicates the high risk of losing prominent facial features of I P when manipulating images in the pixel space. 
本文中的测试标识来自设置2下的测试标识。结果如表2所示。请注意，所有基于CNN的比较方法都是通过学习中间特征实现最佳性能，而我们直接使用“通过生成识别”后的合成图像“程序。性别分类为了进一步证明我们的合成图像在其他面部分析任务中的潜力，我们进行了性别分类的实验。本部分中的所有比较方法也遵循“通过生成识别”程序，其中我们直接使用它们的综合结果进行性别分类。用于性别分类的CNN与编码器Gθg具有相同的结构，并且在UMD [3]数据集的批处理1上训练。我们在表3中报告了Multi-PIE（设置-1）的测试性能。为了公平比较，我们分别以两种分辨率128×128（I P 60）呈现未旋转原始图像的结果。由于标准化视图，TPGAN的合成实现了比原始图像更好的分类精度。所有其他比较模型的性能都比基线差，这并不奇怪，因为它们的架构不是针对性别分类任务而设计的。在[37]中观察到类似的现象，其中合成的高分辨率面部图像严重地使识别性能退化而不是改善它。这表明在操纵像素空间中的图像时丢失I P的突出面部特征的高风险。

128 ) and 60 × 60 (I P 
128）和60×60（I P.

E 
Ë

4.3. Feature Visualization 
4.3。特征可视化

We use t-SNE [34] to visualize the 256-dim deep feature on a two dimensional space. The left side of Fig. 8 illustrates the deep feature space of the original proﬁle images. It’s clear that images with a large pose (90◦ in particular) are not separable in the deep feature space spanned by the Light-CNN. It reveals that even though the LightCNN is trained with millions of images, it still cannot prop
我们使用t-SNE [34]来显示二维空间上的256维深度特征。图8的左侧示出了原始配置图像的深特征空间。很明显，具有大姿势（特别是90°）的图像在Light-CNN跨越的深度特征空间中是不可分离的。它揭示了即使LightCNN训练有数百万张图像，它仍然无法道具

7 
7

Figure 8. Feature space of the proﬁle faces (left) and fontal view synthesized images (right). Each color represents a different identity. Each shape represent a view. The images for one identity are labeled. 
图8.配置文件（左）和平面视图合成图像（右）的特征空间。每种颜色代表不同的身份。每个形状代表一个视图。标记一个身份的图像。

Table 4. Model comparison: Rank-1 recognition rates (%) under Setting 2. 
表4.模型比较：设置2下的Rank-1识别率（％）。

Method w/o P w/o Lip 
方法没有P / o Lip

w/o Ladv w/o Lsym 
没有Ladv w / o Lsym

TP-GAN 
TP-GaN

±90◦ ±75◦ ±60◦ ±45◦ ±30◦ ±15◦ 44.13 66.10 80.64 92.07 96.59 98.35 43.23 56.55 70.99 85.87 93.43 97.06 62.83 76.10 85.04 92.45 96.34 98.09 62.47 75.71 85.23 93.13 96.50 98.47 
±90°±75°±60°±45°±30°±15°44.13 66.10 80.64 92.07 96.59 98.35 43.23 56.55 70.99 85.87 93.43 97.06 62.83 76.10 85.04 92.45 96.34 98.09 62.47 75.71 85.23 93.13 96.50 98.47

98.06 
98.06

98.68 
98.68

64.64 
64.64

77.43 
77.43

87.72 
87.72

95.38 
95.38

erly deal with large pose face recognition problems. On the right side, after frontal view synthesis with our TP-GAN, the generated frontal view images can be easily classiﬁed into different groups according to their identities. 
正确处理大型姿势人脸识别问题。在右侧，使用我们的TP-GAN进行正面视图合成后，生成的正面视图图像可以根据其身份轻松分类到不同的组中。

4.4. Algorithmic analysis 
4.4。算法分析

In this section, we go over different architectures and loss function combinations to gain insight into their respective roles in frontal view synthesis. Both qualitative visualization results and quantitive recognition results are reported for a comprehensive comparison. We compare four variations of TP-GAN in this section, one for comparing the architectures and the other three for comparing the objective functions. Speciﬁcally, we train a network without the local pathway (denoted as P) as the ﬁrst variant. With regards to the loss function, we keep the two-pathway architecture intact and remove one of the three losses, i.e. Lip , Ladv and Lsym , in each case. Detailed recognition performance is reported in Table 4. The two-pathway architecture and the identity preserving loss contribute the most for improving the recognition performance, especially on large pose cases. Although not as much apparent, both the symmetry loss and the adversarial loss help to improve the recognition performance. Fig. 9 illustrates the perceptual performance of these variants. As expected, inference results without the identity preserving loss or the local pathway deviate from the true appearance seriously. And the synthesis without adversarial loss tends to be very blurry, while the result without the symmetry loss sometimes shows unnatural asymmetry effect. 
在本节中，我们将介绍不同的体系结构和损失函数组合，以深入了解它们在正面视图合成中的各自作用。报告定性可视化结果和定量识别结果以进行全面比较。我们比较了本节中TP-GAN的四种变体，一种用于比较体系结构，另一种用于比较目标函数。具体而言，我们训练没有局部路径的网络（表示为P）作为第一个变体。关于损失函数，我们保持双路径结构完整并且在每种情况下去除三个损失中的一个，即Lip，Ladv和Lsym。表4中报告了详细的识别性能。双路径架构和身份保持损失对于提高识别性能贡献最大，尤其是在大型姿势案例中。虽然不那么明显，但对称性损失和对抗性损失都有助于提高识别性能。图9说明了这些变体的感知性能。正如预期的那样，没有身份保留损失或局部路径的推断结果严重偏离了真实的外观。没有对抗性损失的合成趋于非常模糊，而没有对称性损失的结果有时会显示出不自然的不对称效应。

8 
8

(a) methods 
（a）方法

(b) 90◦ 
（b）90°

(c) 75◦ 
（c）75°

(d) 60◦ 
（d）60°

(e) 30◦ 
（e）30°

Figure 9. Model comparison: synthesis results of TP-GAN and its variants. 
图9.模型比较：TP-GAN及其变体的合成结果。

5. Conclusion 
5.结论

In this paper, we have presented a global and local perception GAN framework for frontal view synthesis from a single image. The framework contains two separate pathways, modeling the out-of-plane rotation of the global structure and the non-linear transformation of the local texture respectively. To make the ill-posed synthesis problem well constrained, we further introduce adversarial loss, symmetry loss and identity preserving loss in the training process. Adversarial loss can faithfully discover and guide the synthesis to reside in the data distribution of frontal faces. Symmetry loss can explicitly exploit the symmetry prior to ease the effect of self-occlusion in large pose cases. Moreover, identity preserving loss is incorporated into our framework, so that the synthesis results are not only visually appealing but also readily applicable to accurate face recognition. Experimental results demonstrate that our method not only presents compelling perceptual results but also outperforms state-of-the-art results on large pose face recognition. 
在本文中，我们从单个图像中提出了用于正面视图合成的全局和局部感知GAN框架。该框架包含两个独立的路径，分别建模全局结构的平面外旋转和局部纹理的非线性变换。为了使不适定合成问题得到很好的约束，我们在训练过程中进一步引入了对抗性损失，对称性损失和身份保持损失。对抗性损失可以忠实地发现和引导综合以驻留在正面的数据分布中。在大型姿势情况下，对称性损失可以在减轻自遮挡效应之前明确地利用对称性。此外，身份保护损失被纳入我们的框架，因此合成结果不仅在视觉上吸引人，而且还容易适用于准确的面部识别。实验结果表明，我们的方法不仅具有引人注目的感知结果，而且在大型姿势人脸识别方面也优于最先进的结果。

6. Supplementary Material 
6.补充材料

6.1. Detailed Network Architecture 
6.1。详细的网络架构

Gθg 
Gθg

The detailed structures of the global pathway Gθg and are provided in Table 5 and Table 6. Each convolution layer of Gθg is followed by one residual block [13]. Particularly, the layer conv4 is followed by four blocks. The 
全局路径Gθg的详细结构在表5和表6中提供.Gθg的每个卷积层之后是一个残余块[13]。特别地，层conv4之后是四个块。该

D 
d

E 
Ë

E 
Ë

Figure 10. Our synthesized images present moderately better exposure in some cases. Each tuple consists of three images, with the input I P on the left, the synthesized in the middle, the ground truth frontal face I gt on the right. Each I P and its corresponding I gt are taken under a ﬂash light from the same direction. 
图10.我们的合成图像在某些情况下表现出适度更好的曝光。每个元组由三个图像组成，左边的输入I P，中间的合成，右边的地面真实正面I gt。每个I P及其对应的I gt在相同方向的闪光下拍摄。

Figure 11. Synthesis results under various illuminations. The ﬁrst row is the synthesized image, the second row is the input. Please to refer to the supplementary material for more results. 
图11.各种照明下的合成结果。第一行是合成图像，第二行是输入。请参阅补充材料以获得更多结果。

D 
d

output of the layer f c2 (vid ) is obtained by selecting the maximum element from the two split halves of f c1. The Decoder of the global pathway Gθg contains two parts. The ﬁrst part is a simple deconvolution stack for upsampling the concatenation of the feature vector vid and the random noise vector z . The second part is the main deconvolution stack for reconstruction. Each layer takes the output of its previous layer as the regular input, which is omitted in the table for readability. Any extra inputs are speciﬁed in the I nput column. Particularly, the layers f eat8 and deconv0 have their complete inputs speciﬁed. Those extra inputs instantiate the skipping layers and the bridge between the two pathways. The fused feature tensor from the local pathway is denoted as local in Table 6. Tensor local is the fusion of the outputs of four Gθl s’ layer conv4 (of Table 7). To mix the information of the various inputs, all extra inputs pass through one or two residual blocks before being concatenated for deconvolution. The proﬁle image I P is resized to the corresponding resolution and provides a shortcut access to the original texture for Gθg . Table 7 shows the structures of the local pathway Gθl and Gθl . The local pathway contains three down-sampling and up-sampling processes respectively. The w and h denote the width and the height of the cropped patch. For the patches of the two eyes, we set w and h as 40; for the patch of the nose, we set w as 40 and h as 32; for the patch of the mouth, we set w and h as 48 and 32 respectively. We use rectiﬁed linear units (ReLU) [23] as the nonlinearity activation and adopt batch normalization [15] except for the last layer. In Gθg and Gθl , the leaky ReLU is adopted. Discussion: Our model is simple while achieving better performance in terms of the photorealism of synthesized 
通过从f c1的两个分割半部中选择最大元素来获得层f c2（vid）的输出。全局路径Gθg的解码器包含两部分。第一部分是一个简单的反卷积堆栈，用于对特征向量vid和随机噪声向量z的串联进行上采样。第二部分是重建的主要反卷积堆栈。每个层将其前一层的输出作为常规输入，为了便于阅读，在表中省略了该输出。任何额外的输入都在I nput列中指定。特别是，f eat8和deconv0层具有完整的输入指定。这些额外的输入实例化跳过层和两个路径之间的桥梁。来自局部路径的融合特征张量在表6中表示为局部。张量局部是四个Gθls\x26#39

D 
d

D 
d

D 
d

E 
Ë

E 
Ë

E 
Ë

Table 5. Structure of the Encoder of the global pathway Gθg 
表5.全局路径Gθg的编码器的结构

E 
Ë

Layer conv0 conv1 conv2 conv3 conv4 fc1 fc2 
层conv0 conv1 conv2 conv3 conv4 fc1 fc2

Filter Size 
过滤器尺寸

7 × 7/1 5 × 5/2 3 × 3/2 3 × 3/2 3 × 3/2 
7×7/1 5×5/2 3×3 / 2,3×3×3×3/2




Output Size 
输出尺寸

128 × 128 × 64 64 × 64 × 64 32 × 32 × 128 16 × 16 × 256 8 × 8 × 512 
128×128×64 64×64×64 32×32×128 16×16×256 8×8×512

512 256 
512 256

Table 6. Structure of the Decoder of the global pathway Gθg convs in I nput column refer to those in Table 5. 
表6.全局路径的解码器的结构在输入列中的Gθg转换参考表5中的那些。

D 
d

. The 
。该

Layer feat8 feat32 feat64 feat32 deconv0 deconv1 deconv2 deconv3 conv5 conv6 conv7 
Layer feat8 feat32 feat64 feat32 deconv0 deconv1 deconv2 deconv3 conv5 conv6 conv7

Input fc2, z feat8, conv4 conv3 feat32, conv2, I P feat64, conv1, I P feat128, conv0, local , I P 
输入fc2，z feat8，conv4 conv3 feat32，conv2，I P feat64，conv1，I P feat128，conv0，local，I P

Filter Size 
过滤器尺寸

3 × 3/4 3 × 3/2 3 × 3/2 3 × 3/2 3 × 3/2 3 × 3/2 3 × 3/2 5 × 5/1 3 × 3/1 3 × 3/1 
3×3/4 3×3/2 3×3/3 3×3/3 3×3/3 3×3/3 3×3/2 5×5/3 3×3/3 3×3/1

Output Size 
输出尺寸

8 × 8 × 64 32 × 32 × 32 64 × 64 × 16 128 × 128 × 8 16 × 16 × 512 32 × 32 × 256 64 × 64 × 128 128 × 128 × 64 128 × 128 × 64 128 × 128 × 32 128 × 128 × 3 
8×8×64 32×32×32 64×64×16 128×128×8 16×16×512 32×32×256 64×64×128 128×128×64 128×128×64 128×128×32 128×128×3

Table 7. Structure of the local pathway Gθl in I nput column refer to those in the same table. 
表7.I输入栏中的局部路径Gθ1的结构参考同一表中的那些。

& Gθl 
＆Gθl

E 
Ë

D 
d

. The convs 
。转发

Layer conv0 conv1 conv2 conv3 deconv0 deconv1 deconv2 conv4 conv5 
层conv0 conv1 conv2 conv3 deconv0 deconv1 deconv2 conv4 conv5

Input conv3 conv2 conv1 conv0 
输入conv3 conv2 conv1 conv0

Filter Size 
过滤器尺寸

3 × 3/1 3 × 3/2 3 × 3/2 3 × 3/2 3 × 3/2 3 × 3/2 3 × 3/2 3 × 3/1 3 × 3/1 
3×3/3 3×3/2 3×3/2 3×3/3 3×3/3 3×3/3 3×3/3 3×3/3 3×3/1

Output Size 
输出尺寸

w × h × 64 w/2 × h/2 × 128 w/4 × h/4 × 256 w/8 × h/8 × 512 w/4 × h/4 × 256 w/2 × h/2 × 128 w × h × 64 w × h × 64 w × h × 3 
w×h×64 w / 2×h / 2×128 w / 4×h / 4×256 w / 8×h / 8×512 w / 4×h / 4×256 w / 2×h / 2×128 w×h×64 w×h×64 w×h×3

images. Yim et al. [41] and Zhu et al. [45] use locally connected convolutional layers for feature extraction and fully connected layer for synthesis. We use weight-sharing convolution in most cases. Our model reduces parameter num
图片。 Yim等。 [41]和朱等人。 [45]使用局部连接的卷积层进行特征提取和完全连接的层进行合成。在大多数情况下，我们使用权重共享卷积。我们的模型减少了参数num

9 
9

Figure 12. Synthesis process illustrated from the perspective of activation maps. The up-sampled feature map Cg is combined with the local pathway feature map Cl to produce feature maps with detailed texture. 
图12.从激活图的角度说明的合成过程。上采样特征图Cg与局部路径特征图C1组合以产生具有详细纹理的特征图。

Figure 13. Automatic detection of certain semantic regions. Some skip layers’ activation maps are sensitive to certain semantic regions. One for detecting non-face region is shown on the left, another for detecting hair region is shown on the right. Note the delicate and complex region boundaries around the eyeglasses and the fringe. 
图13.自动检测某些语义区域。一些跳过图层的激活图对某些语义区域敏感。用于检测非面部区域的一个显示在左侧，另一个用于检测头发区域显示在右侧。注意眼镜和边缘周围细腻复杂的区域边界。

bers to a large extent and avoids expensive computation for generating every pixel during synthesis. Yim et al. [41] and Amir et al. [8] add a second reconstruction branch or a reﬁnement network. Our early supervised decoder achieves end-to-end generation of high-resolution image. 
bers在很大程度上避免了在合成期间生成每个像素的昂贵计算。 Yim等。 [41]和Amir等人。 [8]增加第二个重建分支或改进网络。我们早期的监控解码器实现了高分辨率图像的端到端生成。

6.2. Additional Synthesis Results 
6.2。其他综合结果

Additional synthesized images I pred are shown in Fig. 10 and Fig. 11. Under extreme illumination condition, the exposure of I pred is consistent with or moderately better than that of its input I P or its ground truth frontal face I gt . Fig. 11 demonstrates TP-GAN’s robustness to illumination changes. Despite extreme illumination variations, the skin tone, global structure and local details are consistent across illuminations. Our method can automatically adjust I P ’s exposure and white balance. Additionally, we use a state-of-the-art face alignment method [42] to provide four landmarks for TP-GAN under extreme poses. The result is only slightly worse than that reported in Table 2 of the paper. Speciﬁcally, we achieve Rank-1 recognition rates of 87.63(±60◦ ), 76.69(±75◦ ), 62.43(±90◦ ). 
I pred的附加合成图像显示在图10和图11中。在极端照明条件下，I pred的曝光与其输入I P或其地面实况正面I gt的曝光一致或稍微好一些。图11展示了TP-GAN对光照变化的稳健性。尽管极端的照明变化，但是肤色，全局结构和局部细节在整个照明中是一致的。我们的方法可以自动调整I P的曝光和白平衡。此外，我们使用最先进的面部对齐方法[42]为极端姿势下的TP-GAN提供四个标志。结果仅比本文表2中报告的略差。具体而言，我们的Rank-1识别率达到87.63（±60°），76.69（±75°），62.43（±90°）。

6.3. Activation Maps Visualization 
6.3。激活地图可视化

In this part, we visualize the intermediate feature maps to gain some insights into the processing mechanism of the two-pathway network. Fig. 12 illustrates the fusion of global and local information before the ﬁnal output. Cg contains the up-sampled outputs of the global pathway and Cl refers to the features maps fused from the four local pathways. Their information is concatenated and further integrated by the following convolutional layers. 
在这一部分中，我们可视化中间特征图，以获得对双路径网络处理机制的一些见解。图12说明了在最终输出之前的全局和局部信息的融合。 Cg包含全局路径的上采样输出，C1指的是从四个本地路径融合的特征图。它们的信息通过以下卷积层连接并进一步集成。

10 
10

We also discovered that TP-GAN can automatically detect certain semantic regions. Fig. 13 shows that certain skip layers have high activation for regions such as nonface region and hair region. The detection is learned by the network without supervision. Intuitively, dividing the input image into different semantic regions simpliﬁes the following composition or synthesis of the frontal face. 
我们还发现TP-GAN可以自动检测某些语义区域。图13示出了某些跳过层对诸如非脸区域和头发区域的区域具有高激活。网络在没有监督的情况下学习检测。直观地，将输入图像划分成不同的语义区域简化了正面的以下组成或合成。

Acknowledgement 
承认

This work is partially funded by National Natural Science Foundation of China (Grant No. 61622310, 61473289) and the State Key Development Program (Grant No. 2016YFB1001001). We thank Xiang Wu for useful discussion. 
这项工作部分由国家自然科学基金（批准号61622310,61473289）和国家重点发展计划（批准号2016YFB1001001）资助。我们感谢吴翔的有益讨论。

References 
参考

[1] M. Abadi et al. Tensorﬂow: A system for large-scale machine learning. In OSDI, pages 265–283, 2016. 6 [2] T. Ahonen, A. Hadid, and M. Pietikainen. Face description with local binary patterns: Application to face recognition. TPAMI, 2006. 1 [3] A. Bansal, A. Nanduri, R. Ranjan, C. D. Castillo, and R. Chellappa. Umdfaces: An annotated face dataset for training deep networks. arXiv:1611.01484, 2016. 7 [4] D. Chen, X. Cao, F. Wen, and J. Sun. Blessing of dimensionality: High-dimensional feature and its efﬁcient compression for face veriﬁcation. In CVPR, 2013. 1 [5] J. G. Daugman. Uncertainty relation for resolution in space, spatial frequency, and orientation optimized by twodimensional visual cortical ﬁlters. JOSA, 1985. 1 [6] E. L. Denton, S. Chintala, R. Fergus, et al. Deep generative image models using a laplacian pyramid of adversarial networks. In NIPS, 2015. 2 [7] C. Ding and D. Tao. Pose-invariant face recognition with homography-based normalization. Pattern Recognition, 66:144 – 152, 2017. 6 [8] A. Ghodrati, X. Jia, M. Pedersoli, and T. Tuytelaars. Towards automatic image editing: Learning to see another you. In BMVC, 2016. 5, 6, 7, 10 [9] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In NIPS, 2014. 2, 4 [10] R. Gross, I. Matthews, J. Cohn, T. Kanade, and S. Baker. Multi-pie. Image Vision Computing, 2010. 6 [11] Y. Guo, L. Zhang, Y. Hu, X. He, and J. Gao. Ms-celeb-1m: A dataset and benchmark for large-scale face recognition. In ECCV, 2016. 6 [12] T. Hassner, S. Harel, E. Paz, and R. Enbar. Effective face frontalization in unconstrained images. In CVPR, 2015. 2, 3, 5, 6, 7 [13] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770– 778, June 2016. 8 
[1] M. Abadi等。 Tensor fl ow：用于大规模机器学习的系统。在OSDI，第265-283页，2016年.6 [2] T. Ahonen，A。Hadid和M. Pietikainen。具有局部二进制模式的面部描述：面部识别的应用。 TPAMI，2006。1 [3] A. Bansal，A。Nanduri，R。Ranjan，C。D. Castillo和R. Chellappa。 Umdfaces：用于训练深层网络的带注释的面部数据集。 arXiv：1611.01484,2016。7 [4] D. Chen，X。Cao，F。Wen和J. Sun.维度的祝福：高维特征及其对面部验证的有效压缩。在CVPR，2013年.1 [5] J. G. Daugman。通过二维视觉皮层滤波器优化的空间分辨率，空间频率和方向的不确定关系。 JOSA，1985。1 [6] E. L. Denton，S。Chintala，R。Fergus，et al。使用拉普拉斯金字塔的对抗性网络的深度生成图像模型。 NIPS，2015。2 [7] C. Ding和D. Tao。具有基于单应性的归一化的姿势不变的面部识别。模式识别，66：144-152,2017。6 [8] A. Ghodrati，X。Jia，M。Pedersoli和T. Tuytelaars。走向自动图像编辑：学习另一个你。在BMVC，2016年.5,6,7,10 [9] I.Goodfellow，J。Pouget-Abadie，M。Mirza，B。Xu，D。Warde-Farley，S。Ozair，A。Courville和Y. Bengio。生成对抗网。在NIPS，2014年.2,4 [10] R.Gross，I。Matthews，J。Cohn，T。Kanade和S. Baker。多馅饼。 Image Vision Computing，2010。6 [11] Y. Guo，L。Zhang，Y。Hu，X。He和J. Gao。 Ms-celeb-1m：大规模人脸识别的数据集和基准。在ECCV，2016年.6 [12] T. Hassner，S。Harel，E。Paz和R. Enbar。无约束图像中的有效面部正面化。在CVPR，2015年.2,3,5,6,7 [13] K. He，X。Zhang，S。Ren和J. Sun.深度残差学习用于图像识别。 2016年IEEE计算机视觉和模式识别会议（CVPR），第770-778页，2016年6月

[34] L. van der Maaten and G. E. Hinton. Visualizing highdimensional data using t-sne. JMLR, 2008. 7 [35] P. Viola and M. Jones. Rapid object detection using a boosted cascade of simple features. In CVPR, 2001. 1 [36] K. Q. Weinberger and L. K. Saul. Distance metric learning for large margin nearest neighbor classiﬁcation. JMLR, 2009. 1 [37] J. Wu, S. Ding, W. Xu, and H. Chao. Deep joint face hallucination and recognition. arXiv:1611.08091, 2016. 7 [38] X. Wu, R. He, Z. Sun, and T. Tan. A light cnn for deep face representation with noisy labels. arXiv:1511.02683, 2016. 5, 6, 7 [39] C. Xiong, X. Zhao, D. Tang, K. Jayashree, S. Yan, and T. K. Kim. Conditional convolutional neural network for modality-aware face recognition. In ICCV, 2015. 6, 7 [40] J. Yang, S. E. Reed, M.-H. Yang, and H. Lee. Weaklysupervised disentangling with recurrent transformations for 3d view synthesis. In NIPS, 2015. 3 [41] J. Yim, H. Jung, B. Yoo, C. Choi, D. Park, and J. Kim. Rotating your face using multi-task deep neural network. In CVPR, 2015. 2, 3, 5, 6, 7, 9, 10 [42] H. Zhang, Q. Li, and Z. Sun. Combining data-driven and model-driven methods for robust facial landmark detection. arXiv:1611.10152, 2016. 10 [43] Z. Zheng, L. Zheng, and Y. Yang. Unlabeled samples generated by gan improve the person re-identiﬁcation baseline in vitro. arXiv:1701.07717, 2017. 3 [44] X. Zhu, Z. Lei, J. Yan, D. Yi, and S. Z. Li. High-ﬁdelity pose and expression normalization for face recognition in the wild. In CVPR, 2015. 2, 3, 5, 6, 7 [45] Z. Zhu, P. Luo, X. Wang, and X. Tang. Deep learning identity-preserving face space. In ICCV, 2013. 1, 2, 3, 6, 7, 9 [46] Z. Zhu, P. Luo, X. Wang, and X. Tang. Multi-view perceptron: a deep model for learning face identity and view representations. In NIPS, 2014. 1, 2, 3, 7 
[34] L. van der Maaten和G. E. Hinton。使用t-sne可视化高维数据。 JMLR，2008。7 [35] P. Viola和M. Jones。使用增强级联的简单功能进行快速物体检测。在CVPR，2001年.1 [36] K. Q. Weinberger和L. K. Saul。大边距最近邻分类的距离度量学习。 JMLR，2009。1 [37] J. Wu，S。Ding，W。Xu和H. Chao。深部关节面部幻觉和识别。 arXiv：1611.08091,2016。7 [38] X. Wu，R。He，Z。Sun和T. Tan。用于深度表示的浅色cnn，带有嘈杂的标签。 arXiv：1511.02683,2016.5,6,7 [39] C. Xiong，X。Zhao，D。Tang，K。Jayashree，S。Yan和T. K. Kim。用于模态感知人脸识别的条件卷积神经网络。在ICCV，2015。6,7 [40] J. Yang，S。E. Reed，M.-H。杨和李。李。弱视监督与三维视图合成的循环变换解开。在NIPS，2015年.3 [41] J. Yim，H。Jung，B。Yoo，C。Choi，D。Park和J. Kim。使用多任务深度神经网络旋转你的脸。在CVPR，2015年.2,3,5,6,7,9,10 [42] H. Zhang，Q。Li和Z. Sun.结合数据驱动和模型驱动方法，实现稳健的面部标志检测。 arXiv：1611.10152,2016。10 [43] Z. Zheng，L。Zheng和Y. Yang。由gan生成的未标记样本在体外改善了人的再鉴定基线。 arXiv：1701.07717,2017。3 [44] X. Zhu，Z。Lei，J。Yan，D。Yi和S. Z. Li。野外人脸识别的高保真姿态和表情归一化。在CVPR，2015年.2,3,5,6,7 [45] Z. Zhu，P。Luo，X。Wang和X. Tang。深度学习保持身份的面部空间。在ICCV，2013.1,2,3,6,7,9 [46] Z. Zhu，P。Luo，X。Wang和X. Tang。多视图感知器：用于学习面部识别和视图表示的深层模型。在NIPS，2014年.1,2,3,7

[14] G. B. Huang, M. Ramesh, T. Berg, and E. Learned-Miller. Labeled faces in the wild: A database for studying face recognition in unconstrained environments. Technical Report 07-49, University of Massachusetts, Amherst, October 2007. 6 [15] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv:1502.03167, 2015. 9 [16] J. Johnson, A. Alahi, and L. Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In ECCV, 2016. 2, 5 [17] M. Kan, S. Shan, H. Chang, and X. Chen. Stacked progressive auto-encoders (spae) for face recognition across poses. In CVPR, 2014. 2, 3 [18] D. P. Kingma and M. Welling. Auto-encoding variational bayes. In ICLR, 2014. 2 [19] C. Ledig et al. Photo-realistic single image super-resolution using a generative adversarial network. In CVPR, 2017. 2, 3 [20] C. Li and M. Wand. Combining markov random ﬁelds and convolutional neural networks for image synthesis. In CVPR, 2016. 3 [21] M. Mirza and S. Osindero. Conditional generative adversarial nets. arXiv:1411.1784, 2014. 3 [22] U. Mohammed, S. J. Prince, and J. Kautz. Visio-lization: generating novel facial images. In TOG, 2009. 4 [23] V. Nair and G. E. Hinton. Rectiﬁed linear units improve restricted boltzmann machines. In J. Frnkranz and T. Joachims, editors, Proceedings of the 27th International Conference on Machine Learning (ICML-10), pages 807–814. Omnipress, 2010. 9 [24] D. Pathak, P. Krahenbuhl, J. Donahue, T. Darrell, and A. A. Efros. Context encoders: Feature learning by inpainting. In CVPR, 2016. 2, 3 [25] S. Reed, K. Sohn, Y. Zhang, and H. Lee. Learning to disentangle factors of variation with manifold interaction. In ICML, 2014. 2 [26] D. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In ICML, 2014. 2 [27] C. Sagonas, Y. Panagakis, S. Zafeiriou, and M. Pantic. Robust statistical face frontalization. In ICCV, 2015. 2 [28] F. Schroff, D. Kalenichenko, and J. Philbin. Facenet: A uniﬁed embedding for face recognition and clustering. In CVPR, 2015. 1 [29] W. Shen and R. Liu. Learning residual images for face attribute manipulation. In CVPR, 2017. 3 [30] A. Shrivastava, T. Pﬁster, O. Tuzel, J. Susskind, W. Wang, and R. Webb. Learning from simulated and unsupervised images through adversarial training. In CVPR, 2017. 3, 4 [31] Y. Sun, X. Wang, and X. Tang. Deep learning face representation from predicting 10,000 classes. In CVPR, 2014. 1, 2 [32] Y. Taigman, M. Yang, M. Ranzato, and L. Wolf. Deepface: Closing the gap to human-level performance in face veriﬁcation. In CVPR, 2014. 1, 2 [33] L. Tran, X. Yin, and X. Liu. Disentangled representation learning gan for pose-invariant face recognition. In CVPR, 2017. 4, 5, 7 
[14] G. B. Huang，M。Ramesh，T。Berg和E. Learned-Miller。野外标记的面孔：用于在无约束环境中研究面部识别的数据库。技术报告07-49，马萨诸塞大学阿默斯特分校，2007年10月.6 [15] S. Ioffe和C. Szegedy。批量标准化：通过减少内部协变量偏移来加速深度网络训练。 arXiv：1502.03167,2015。9 [16] J. Johnson，A。Alahi和L. Fei-Fei。实时样式传输和超分辨率的感知损失。在ECCV，2016.2,5 [17] M. Kan，S。Shan，H。Chang和X. Chen。堆叠式渐进式自动编码器（spae），用于跨姿势进行面部识别。在CVPR，2014年.2,3 [18] D. P. Kingma和M. Welling。自动编码变分贝叶斯。在ICLR，2014。2 [19] C.Ledig等。使用生成对抗网络的照片般逼真的单图像超分辨率。在CVPR，2017年.2,3 [20] C. Li和M. Wand。结合马尔可夫随机场和卷积神经网络进行图像合成。在CVPR，2016年.3 [21] M.米尔扎和S. Osindero。有条件的生成对抗网。 arXiv：1411.1784,2014。3 [22] U. Mohammed，S。J. Prince和J. Kautz。 Visio化：产生新颖的面部图像。在TOG，2009年.4 [23] V. Nair和G. E. Hinton。 Recti fi ed ed线性单元改进了受限制的boltzmann机器。在J.Frnkranz和T.Joachims，编辑，第27届国际机器学习会议论文集（ICML-10），第807-814页。 Omnipress，2010。9 [24] D. Pathak，P。Krahenbuhl，J。Donahue，T。Darrell和A. A. Efros。上下文编码器：通过修复进行特征学习。在CVPR，2016年.2,3 [25] S. Reed，K。Sohn，Y。Zhang和H. Lee。学习通过多种相互作用来解开变异因素。在ICML，2014。2 [26] D. J. Rezende，S。Mohamed和D. Wierstra。深部生成模型中的随机反向传播和近似推断。在ICML，2014。2 [27] C. Sagonas，Y。Panagakis，S。Zafeiriou和M. Pantic。强大的统计面部正面化。在ICCV，2015年.2 [28] F. Schroff，D。Kalenichenko和J. Philbin。 Facenet：用于人脸识别和聚类的统一嵌入。在CVPR，2015年.1 [29] W. Shen和R. Liu。学习残差图像以进行面部属性操作。在CVPR，2017年.3 [30] A. Shrivastava，T。P fi ster，O。Tuzel，J。Susskind，W。Wang和R. Webb。通过对抗训练学习模拟和无监督图像。在CVPR，2017年.3,4 [31] Y. Sun，X。Wang和X. Tang。通过预测10,000个班级深入学习面部表征。在CVPR，2014年.1,2 [32] Y. Taigman，M。Yang，M。Ranzato和L. Wolf。深入研究：在面部验证中缩小与人类绩效的差距。在CVPR，2014年.1,2 [33] L. Tran，X。Yin和X. Liu。解缠表示学习gan用于姿势不变的人脸识别。在CVPR，2017年.4,5,7

11 
11

